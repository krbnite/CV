{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .backtick {\n",
       "            font-style:oblique;\n",
       "            font-family:\"Courier New\";\n",
       "            font-size:15px;\n",
       "            padding-left:-5px;\n",
       "        }\n",
       "    </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "html = HTML(\"\"\"\n",
    "    <style>\n",
    "        .backtick {\n",
    "            font-style:oblique;\n",
    "            font-family:\"Courier New\";\n",
    "            font-size:15px;\n",
    "        }\n",
    "    </style>\n",
    "\"\"\")\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE FROM THE FUTURE (2019-Dec-16)**\n",
    "\n",
    "<div style=\"background-color:sandybrown;padding-left:5px;\">\n",
    "    <p> This work was from when we first took the project over from Udi (it began 2019-Oct-23, but the \n",
    "        majority of it was done on 2019-Nov-01).  </p>\n",
    "    <p> What this means is:\n",
    "    <ol>\n",
    "        <li>the assumption is Udi's data processing pipeline (which extends the first half\n",
    "            of Carlos' original pipeline) works well enough</li>\n",
    "        <li>therefore, Udi's final outputted data table from this pipeline can be used for analysis.  </li>\n",
    "    </ol></p>\n",
    "    <p> At one point, Dani and I began questioning these assumptions and started everything from scratch\n",
    "        (this explains the mbh001_v3 directory in the MBH001 GitLab repo, where I will also store a copy\n",
    "        of this JNB for convenience).  </p>\n",
    "    <p style=\"padding-left:40px;\"> <b>SIDE NOTE</b>: Starting from scratch was nontrivial since the full data set\n",
    "        derives from about 40 tables, some of which (as we found) are not usable or valid data.  These tables\n",
    "        host a few thousand variables -- again, many of which need to be understood and vetted.  I must say,\n",
    "        though we originally had (harshly) questioned Carlos' methods (some of which perhaps \n",
    "        deserved it) and subsequently\n",
    "        Udi's methods (many of which just built on top of Carlos'), we ultimately could not \"force\" \n",
    "        the data to do any better than Udi or Carlos could.  It's possible that the strategy and methods\n",
    "        used by Udi and Carlos would have borne fruit if there was enough fruit to bear.  (More importantly,\n",
    "        for better or worse, it's definitely possible that their methods would have been left unquestioned\n",
    "        if they were able to capture some signal, independent of whether their methods were janky\n",
    "        or suboptimal.)</p>   \n",
    "    <p> Anyway, in this notebook, I explore methods that I had hoped would help \"stabilize\" the patient\n",
    "        population between training, validation, and test sets. </p>\n",
    "    <p> <b>MOTIVATION</b>:</p>\n",
    "    <p> Basically, the first thing you will\n",
    "        notice when working with this data is that it's very hard to not overfit to training. This is\n",
    "        sometimes due to lack or regularization, but other times the problem can start earlier on in\n",
    "        the pipeline.  For example, in a\n",
    "        highly-imbalanced binary classification problem, issues like this can be due to a bad\n",
    "        splitting strategy (e.g., if you have a 1% event rate, its possible that a naive splitting strategy\n",
    "        will accidentally land all your events in the validation and test sets, making training\n",
    "        pointless and fruitless).  Though we are doing a regression problem this motivates my major idea. </p>\n",
    "    <p> <b>IDEA</b>:</p>\n",
    "    <p> It is likely that a poor/naive splitting strategy can affect a\n",
    "        regresion problem as well, so I adapt the concept of stratified splitting from a categorical\n",
    "        target variable to a continuous target.</p>    \n",
    "    <p> <b>RESULTS</b>:</p> \n",
    "        <p><ul>\n",
    "            <li> Overall, it seems to help a very little bit, but does not really save the day.</li>\n",
    "            <li> Importantly, what I found is that any observed performance boosts are almost always \n",
    "                due to the random seed used to split the data.  To me, this means that there is some\n",
    "                opportunistic splitting that we do not understand.  That is, though stratifying on the\n",
    "                continuous target variable did not prevent this issue, there must be some variable\n",
    "                that does -- otherwise a random \"random seed\" wouldn't suddenly see a performance boost\n",
    "                for no reason.  One of our goals should be to try to identify this feature, if possible. </li>\n",
    "        </ul>\n",
    "    </p>\n",
    "    <hr>\n",
    "    <p style='background-color:orange;'> \n",
    "        <b>NOTE</b>: For analyses relevant to final report sent to Gates Foundation, please\n",
    "        check out notebooks <font class=backtick>01_KU</font> through \n",
    "        <font class=backtick>04_KU</font> in <font class=backtick>mbh001/mbh001_v3</font>.\n",
    "    </p>\n",
    "    <hr>\n",
    "    <p style='color:red;background-color:yellow;margin-left:-5px'><b>NOTE</b>: This notebook is copied to \n",
    "         <font class=backtick>mbh001_v3/notebooks</font>\n",
    "         where it is prefixed <font class=backtick>00c__KU</font>.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "--------------------------------------------------\n",
    "\n",
    "\n",
    "**2019-Oct-23**\n",
    "\n",
    "Since this project has changed hands quite a bit, I am just doing some exploratory analysis here.\n",
    "\n",
    "# Motivation \n",
    "\n",
    "If you look at the model runs when we've removed patients that had \"cesarian w/o labor\", the R2 score on training is often double the R2 on the dev/val set. For example, for a LASSO model trained on everything but the sensor data, we saw:\n",
    "\n",
    "```\n",
    "# Clinical+Demographic Data (special patients removed from data set)\n",
    "R2 train:  0.74\n",
    "R2 dev: 0.25\n",
    "```\n",
    "\n",
    "For another LASSO run w/ all the data, including the sensor stuff, this effect was reduced, but\n",
    "not eliminated:\n",
    "\n",
    "```\n",
    "# All Data (special patients removed from data set)\n",
    "R2 train: 0.37\n",
    "R2 dev: 0.24\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "This is not the case when these patients are not removed.  For example, when training a LASSO model with the clinical and demographic data, we found:\n",
    "\n",
    "```\n",
    "# Clinical+Demographic Data (special patients included)\n",
    "R2 train: 0.33\n",
    "R2 dev: 0.36\n",
    "```\n",
    "\n",
    "\n",
    "If there is anything surprising when the special patients are included, it is that R2 is higher on the dev \n",
    "set.  Otherwise, these numbers are close enough and look like what you would expect (for a model that is not over/under fit).  \n",
    "\n",
    "\n",
    "## Discussion\n",
    "Ok, let's think about this: R2 (\"R squared\" or \"R-two-score\") is not a great way to measure\n",
    "the predictive power of a model.  I'll get to why.\n",
    "\n",
    "R2 is often quoted as a measure of \"goodness of fit.\" It says something about how much \n",
    "of the variance in a given data set is explained by the model on that given data set.  Everything\n",
    "still sounds ok, right?  But it's not: the definition is data set dependent.  \n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2}{\\sum_{i=1}^{n}(y_{i}-\\bar{y}_{i})^2}\n",
    "$$\n",
    "\n",
    "Notice that denominator.  It's called the \"null model.\" It measures how well you can\n",
    "predict outcomes if the only information you had was a data set representative of\n",
    "the true population and an unbiased estimate of the mean.  \n",
    "\n",
    "This should give you pause about using the R2 score to compare training and dev sets.  \n",
    "\n",
    "\n",
    "### Scenario 1:  Dependence on Outcome Range\n",
    "Assume that the independent variable, x, is time at a monthly interval and the\n",
    "dependent variable, y, is the monthly revenue your firm has acquired.  You plot\n",
    "y vs x and see that it is noisy, but there has been consistent growth month to \n",
    "month -- and much of that growth look fairly linear.  You get the idea to do\n",
    "a regression to start making predictions for your boss...  The model fit\n",
    "looks good on the plot, but to be sure you compute the R2 score.\n",
    "\n",
    "About a year later, your boss asks whether the model is still working well.  So \n",
    "you re-compute the R2 score, this time using\n",
    "the original data plus the new data from the past year.  What you find is\n",
    "a great surprise: \"The model,\" you tell your boss, \"not only still works, but\n",
    "is working better than ever before!\"\n",
    "\n",
    "Let's look at the definition of R2 again:\n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2}{\\sum_{i=1}^{n}(y_{i}-\\bar{y}_{i})^2}\n",
    "$$\n",
    "\n",
    "Note that this can be recast in terms of mean squared error (MSE; more on this below):\n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\frac{MSE_{pred}}{MSE_{null}}\n",
    "$$\n",
    "\n",
    "Here, `MSE_{pred}` is the MSE of your predictive model, while `MSE_{null}` is the MSE of the\n",
    "null model.  \n",
    "\n",
    "Let's assume that the same regression still fits the data, new and old, very\n",
    "well.  In fact, let's say you know this for sure because you refit the model and\n",
    "all the coefficients were more-or-less the same as before.  \n",
    "\n",
    "However, your R2 score is bigger.  Why?\n",
    "\n",
    "You investigate.  First, you look at `MSE_{pred}` on the original data and compare it\n",
    "to its computation on all the data (original and newly collected).  You note that\n",
    "it hasn't changed much.  In other words, `MSE_{pred}` appears to be fairly constant.  Well,\n",
    "this leaves only one possibility: `MSE_{null}` must have grown.\n",
    "\n",
    "Well, duh!  \n",
    "\n",
    "Think about what happened:  monthly revenue continued to grow each month, so the range\n",
    "of the data has increased.  This means that (i) there will be a new estimate of the mean, \n",
    "(ii) that outcome values will have a larger spread about this mean, (iii) that `MSE_{null}`\n",
    "will in turn be a larger value than before, which (iv) means that `MSE_{pred}/MSE_{null}` will\n",
    "have shrunk, which (v) means R2 will have grown.\n",
    "\n",
    "\n",
    "Huge takeaway:  In this scenario, MSE remained constant over the two data sets, properly\n",
    "reflecting that the model still worked well, but no better than it did previously. \n",
    "\n",
    "\n",
    "\n",
    "### Scenario 2:  Dependence on Range (Take 2)\n",
    "\n",
    "In this scenario, imagine you think you're doing everything right.  For example, you split the data\n",
    "into training, development, and test sets.  \n",
    "\n",
    "You fit the model to the training set, then compute R2 on training and dev.  On training things look\n",
    "great, but on dev it's not so good.  You try another type of model, but find the same thing.  You\n",
    "do this several times, but continue having a discrepancy... \n",
    "\n",
    "What is happening?  \n",
    "\n",
    "First thing that might come to mind is that the data was poorly split -- that the training and\n",
    "dev sets do not really represent the same outcome population.  You look at the training set and\n",
    "dev set means: they look about the same.  So you compute medians...again, about the same.  Then\n",
    "you look at min and max.  \n",
    "\n",
    "The dev set has a much tighter distribution than the training set.\n",
    "\n",
    "Ah, now it all makes sense:  the tighter distribution around the same mean indicates that\n",
    "$MSE_{null,dev} < MSE_{null,trn}$, which means $R^{2}_{dev} < R^{2}_{trn}$\n",
    "\n",
    "In this scenario, you might still find comparable MSEs for the predictive model itself.\n",
    "\n",
    "The major point thus far:  if you want to use R2 score, then the definition of R2 score\n",
    "should be modified, while respecting best practices of predictive modeling.\n",
    "\n",
    "Bad:\n",
    "* compute null model on full data set (data leakage)\n",
    "\n",
    "Good:\n",
    "* compute null model on training set\n",
    "* use same null model for R2 computations on training, dev, test\n",
    "\n",
    "\n",
    "At least with this modified defintion of R2, we would no longer have ambiguities arise\n",
    "that have nothing to do with the type of predictive power we're interested in.  It makes\n",
    "more sense for predictive modeling, where the dev and test data are at various levels\n",
    "of out-of-sight-out-of-mind.  The idea is that the training null model is our \"baseline\n",
    "bad model\" that we all validate on dev and test sets.\n",
    "\n",
    "However, the better idea is just to do what most machine learning practitioners actually\n",
    "do in this case:  use RMSE instead of R2.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "\n",
    "For \n",
    "training, you are looking at how much better a prediction you've\n",
    "made than just guessing the mean of the training set.  \n",
    "\n",
    "For dev, you  are looking at how much better a \n",
    "prediction you've made than just guessing the mean of the dev set.  \n",
    "\n",
    "Well, this is only ok if \n",
    "the estimates of the target mean on the two sets are comparable.  This is partially why I would suspect\n",
    "that the training and dev sets differ in their target distribution after we removed the patients \n",
    "having \"cesarian w/o labor.\"  This is also why I'd argue to use MSE for a predicive model, not\n",
    "R2 (which is used by statisticians building inference models without a val/dev set).\n",
    "\n",
    "\n",
    "## Bla\n",
    "\n",
    "For predictive regression models, I have more intuition using MSE.  If the MSE is much larger on the dev\n",
    "set than the training set, I'd wager that the model has overfit to the training set.  Alternatively, I might\n",
    "guess that the dev set differs in its target distribution (this is easy to imagine in classification models\n",
    "where low frequency classes might be left out of one of the set without using stratified sampling).  \n",
    "\n",
    "The definition of R2 score is just a linear transformation of MSE, so it should behave similarly to MSE (in a linearly transformed way).  One difference is that a bigger number (closer to 1.0) is better than a smaller\n",
    "number (closer to 0.0).  \n",
    "\n",
    "MSE: \n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^{2}\n",
    "$$\n",
    "\n",
    "R2 Score: \n",
    "$$\n",
    "R^{2} = 1 - \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2}{\\sum_{i=1}^{n}(y_{i}-\\bar{y}_{i})^2} \\\\\n",
    "\\quad = 1 - \\frac{N*MSE}{(N-1)\\sigma^{2}} \\\\\n",
    "\\quad\\quad\\quad\\quad = \\frac{N}{(N-1)\\sigma^{2}}MSE + (-1) \\\\\n",
    "\\quad = m*MSE + b\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Approach\n",
    "We must first define what we mean by similar enough.  \n",
    "\n",
    "In classification, this means we have enough of each class in train/dev/test.  \n",
    "\n",
    "For regression, it might mean we have enough representation over each interval.  As usual, someone already came across this issue and solved it.  For example, in this blog, the author provides a small snippet of code that does the trick.  Essentially, you create a new, binned version of the continuous target variable, then stratify on the binned variable – after which, you can just throw it out.  Very simple!\n",
    "\n",
    "```python\n",
    "# Create the bins.  My `y` variable has\n",
    "# 506 observations, and I want 50 bins.\n",
    "\n",
    "bins = np.linspace(0, 506, 50)\n",
    "\n",
    "# Save your Y values in a new ndarray,\n",
    "# broken down by the bins created above.\n",
    "\n",
    "y_binned = np.digitize(y, bins)\n",
    "\n",
    "# Pass y_binned to the stratify argument,\n",
    "# and sklearn will handle the rest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, stratify=y_binned)\n",
    "\n",
    "```\n",
    "\n",
    "Before I do something like that though, I just want to look at the actual distributions between Udi’s training and dev sets.\n",
    "\n",
    "\n",
    "\n",
    "## Other Criticisms of R2\n",
    "It is well known that R2 can be a horrible way to compare different models, especially\n",
    "if differing amounts of variables are used.  For example, assumes we maintain the\n",
    "same model architecture, but continue adding variables: R2 will inevitably increase w/\n",
    "no evidence of whether the new variables actually helped or hurt: is the model fitting to\n",
    "the noise, or has a real relationship been found?   This is where a validation set becomes \n",
    "necessary -- well, from a predictive modeling perspective it's always necessary, but \n",
    "traditional statisticians did not use validation sets.  The validation set will\n",
    "likely show you big differences in R2 if the model was fitting to the noise, however\n",
    "due to criticisms outlined above, the change in R2 between data sets is not always\n",
    "unambiguous...  Better to use adjusted R2.  Or, if caring about predictions, maybe\n",
    "just use RMSE w/ both training and dev/val sets.  There is also something called predictive R2, but this is again\n",
    "from a time when validation sets weren't used: its based on a leave-one-out CV \n",
    "strategy.\n",
    "\n",
    "\n",
    "## Last Note\n",
    "I found that John Myles White said this all more eloquently than I've spelled out:\n",
    "\n",
    "> People sometimes use R2 as their preferred measure of model fit. Unlike quantities such as MSE or MAD, R2 is not a function only of model’s errors, its definition contains an implicit model comparison between the model being analyzed and the constant model that uses only the observed mean to make predictions. As such, R2 answers the question: “does my model perform better than a constant model?” But we often would like to answer a very different question: “does my model perform worse than the true model?”\n",
    ">\n",
    "> In theoretical examples, it is easy to see that the answers to these two questions are not interchangeable. We can construct examples in which our model performs no better than a constant model, even though it also performs no worse than the true model. But we can also construct examples in which our model performs much better than a constant model, even when it also performs much worse than the true model.\n",
    ">\n",
    "> As with all model comparisons, R2 is a function not only of the models being compared, but also a function of the data set being used to perform the comparison. For almost all models, there exist data sets that are wholly incapable of distinguishing between the constant model and the true model. In particular, when using a dataset with insufficient model-discriminating power, R2 can be pushed arbitrarily close to zero — even when we are measuring R2 for the true model. As such, we must always keep in mind that R2 does not tell us whether our model is a good approximation to the true model: R2 only tells us whether our model performs noticeably better than the constant model on our dataset.\n",
    "- [Why I'm Not a Fan of R2](http://www.johnmyleswhite.com/notebook/2016/07/23/why-im-not-a-fan-of-r-squared/), J. M. White\n",
    "\n",
    "\n",
    "Apparently, R2 is not really great for causal inference either...e.g., check \n",
    "[this](https://thestatsgeek.com/2014/01/25/r-squared-and-goodness-of-fit-in-linear-regression/) out.  Turns\n",
    "out that, other than being a super common statistic that people think they have intuition for, it's not\n",
    "all that good for anything...\n",
    "\n",
    "\n",
    "https://stats.stackexchange.com/questions/214886/regression-what-is-the-utility-of-r-squared-compared-to-rmse\n",
    "\n",
    "https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/\n",
    "\n",
    "\n",
    "\n",
    "Here is someone who asks about R squared on the validation set:  https://www.reddit.com/r/rstats/comments/3al070/question_about_out_of_sample_rsquared/.  Basically,\n",
    "the first commenter says what I consider to be wrong.  The second commenter says what I consider to\n",
    "be right:  the mean should be the mean from the training set b/c you want to compare how well\n",
    "your model performs in validation against how well the null model would have performed in validation.\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "* [Sklearn's R2 Score](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)\n",
    "* [Sklearn's MSE](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)\n",
    "* https://michaeljsanders.com/2017/03/24/stratify-continuous-variable.html\n",
    "* https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4530125/\n",
    "* https://data.library.virginia.edu/is-r-squared-useless/\n",
    "* https://statisticsbyjim.com/regression/interpret-adjusted-r-squared-predicted-r-squared-regression/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Look\n",
    "Here, I just use Udi's train/dev/tst split (same random seed, etc).  Then I look some distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import zscore,expon\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Data\n",
    "d = pickle.load( open( \"../data/processed/mbh001_f3_toi_mon_baseline_a.p\", \"rb\" ))\n",
    "\n",
    "# Nullity Threshold\n",
    "na_th = 0.8\n",
    "\n",
    "# Drop High-Nullity Cols\n",
    "df = d.dropna(axis=1,thresh=len(d)*na_th).copy()\n",
    "\n",
    "# Separate X & Y Data\n",
    "x = df.drop(['time_left','ga.birth'], axis=1)\n",
    "y = df['time_left']\n",
    "\n",
    "# Feature Groups\n",
    "con_feat   = x.select_dtypes('float64').columns\n",
    "cat_feat   = x.select_dtypes('object').columns\n",
    "sensor_feat= x.filter(regex='^toi|^mon').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-483.42412224776774"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[num_feat].min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are using RandomForest, I will NOT OHE cat vars or try\n",
    "#   to \"smartly\" impute continuous vars.  Instead, I'll assign a\n",
    "#   'NaN' category to catVars and a large, unique, negative value\n",
    "#   to contVars\n",
    "# NOTE: even though there are catVars, they are all numerically\n",
    "#   encoded, so creating a 'null' category mixes type; instead,\n",
    "#   just create a -9999 category like with continuous vars.\n",
    "x.fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of \"Time Left\" for Unsplit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1a45bdd8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data in trn/dev/tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of \"Time Left\" for Trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ceae080>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trn.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of \"Time Left\" for Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ab61eb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_dev.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of \"Time Left\" for Tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ad99ba8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_tst.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts So Far\n",
    "Well, interestingly, you can see that the training distribution is better fleshed out and\n",
    "normal looking than the dev or test distributions...so maybe the stratified split will help with this,\n",
    "and maybe that will help with the divergent model R2 scores.\n",
    "\n",
    "First, so everything is in one place in this notebook, let's look at the MSEs and R2 scores for various model\n",
    "runs using these un-stratified splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-Stratified Model Metrics (Full Population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [10,100,500,1000],           # 4\n",
    "    'max_features': [2, 3, 'log2','sqrt', None], # 5\n",
    "    'max_depth': [3,4,8,16,32,None],             # 6\n",
    "    'min_samples_split': [2,4,8,16,32,64],       # 6\n",
    "    'min_samples_leaf': [1,2,5,10,30,0.1,0.2],   # 7\n",
    "} # 2*4*5*6*6*7 = 1,080-model GridSearch\n",
    "# Let's assume that randomly samplng 250 models is good enough\n",
    "n_iter = 250\n",
    "kfolds = 3\n",
    "\n",
    "best_rf = RandomizedSearchCV(\n",
    "    rf,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed: 15.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=250, n_jobs=-1,\n",
       "          param_distributions={'criterion': ['mse', 'mae'], 'n_estimators': [10, 100, 500, 1000], 'max_features': [2, 3, 'log2', 'sqrt', None], 'max_depth': [3, 4, 8, 16, 32, None], 'min_samples_split': [2, 4, 8, 16, 32, 64], 'min_samples_leaf': [1, 2, 5, 10, 30, 0.1, 0.2]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None,\n",
       "          refit='neg_mean_squared_error', return_train_score=True,\n",
       "          scoring=['r2', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error'],\n",
       "          verbose=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.fit(x_trn,y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 8,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': None,\n",
       " 'max_depth': 16,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 3.7806163434903044\n",
      "MAE, Dev: 7.978122844827586 \n",
      "\n",
      "MSE, Trn: 24.09238957040628\n",
      "MSE, Dev: 96.91025222521553 \n",
      "\n",
      "R2, Trn: 0.8309494827850666\n",
      "R2, Dev: 0.343523784765153\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rf.predict(x_trn)\n",
    "y_dev_pred = best_rf.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Thoughts\n",
    "Wow, if you ask me, the model was allowed to completely overfit the training data.  Figuring CV is\n",
    "used, this does make it seem like the training set and dev set must have some fundamental differences,\n",
    "which stratified sampling might help.\n",
    "\n",
    "\n",
    "# Stratified Model Metrics (Full Population, Test)\n",
    "We'll create 5 target bins, where target ranges from 0 to 61.  \n",
    "\n",
    "We stratify properly between training and union(dev,tst), the just split dev and tst.\n",
    "\n",
    "We will use the parameters from the best fit model before, then do a GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1d011be0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_trn.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a23fccef0>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_dev.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2440cac8>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_tst.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfs = RandomForestRegressor(\n",
    "    n_estimators = 1000,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 1,\n",
    "    max_features = None,\n",
    "    max_depth = 16,\n",
    "    criterion = 'mae',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# Crazy: note that I didn't use random state 42 here...and got great results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=16,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=8,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 3.7872987072945525\n",
      "MAE, Dev: 7.4837521551724135 \n",
      "\n",
      "MSE, Trn: 24.241840129501384\n",
      "MSE, Dev: 91.85436157650864 \n",
      "\n",
      "R2, Trn: 0.8276769036013358\n",
      "R2, Dev: 0.31959881987449923\n",
      "R2, Dev: 0.3003842692559925\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = rfs.predict(x_trn)\n",
    "y_dev_pred = rfs.predict(x_dev)\n",
    "y_tst_pred = rfs.predict(x_tst)\n",
    "\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))\n",
    "print('R2, Dev:', r2_score(y_tst, y_tst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Model Metrics (Full Population, GridSearch)\n",
    "For the first random gridsearch, I will maintain the same hyperparameter sets.  However,\n",
    "I have some concerns...\n",
    "\n",
    "The last time, the GridSearch turned up `max_features: None`, which literally means we no longer\n",
    "have a true RF...and likely have highly correlated trees, which is bound to generalize to dev set\n",
    "poorly.  Also, `max_depth: None` is potentially concerning...  Not necessarily, but some depth\n",
    "regularization is likely a good thing.  To know how much, we have to look at the depths of the \n",
    "trees in the gridSearch's best RF. (Actually, we know it is 16, but I include a code snippet\n",
    "that shows you how to do it in general.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 16., 16., 16., 16.])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([tree.max_depth for tree in best_rf.best_estimator_.estimators_], [0,25,50,75,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Base RF\n",
    "rf_strat = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [10,100,500,1000],           # 4\n",
    "    'max_features': [2, 3, 'log2','sqrt', None], # 5\n",
    "    'max_depth': [3,4,8,16,32,None],             # 6\n",
    "    'min_samples_split': [2,4,8,16,32,64],       # 6\n",
    "    'min_samples_leaf': [1,2,5,10,30,0.1,0.2],   # 7\n",
    "} # 2*4*5*6*6*7 = 10,080-model GridSearch\n",
    "# Let's assume that randomly samplng 250 models is good enough\n",
    "n_iter = 250\n",
    "kfolds = 3\n",
    "\n",
    "# GridSearch\n",
    "best_rfs = RandomizedSearchCV(\n",
    "    rf_strat,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed: 27.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 10,\n",
       " 'max_features': None,\n",
       " 'max_depth': 16,\n",
       " 'criterion': 'mse'}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rfs.fit(x_trn,y_trn)\n",
    "best_rfs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting discrepancies between the non-stratified and stratified runs.\n",
    "\n",
    "\n",
    "|  run     | n_estimators | min_samples_split | min_samples_leaf | max_features | max_depth | criterion |\n",
    "|----------|--------------|-------------------|------------------|--------------|-----------|-----------|\n",
    "| No Strat | 1000         | 8                 | 1                | None         | 16        | MAE       |\n",
    "| W/ Strat | 1000         | 4                 | 10               | None         | 16        | MSE       |\n",
    "\n",
    "What this tells me is:\n",
    "* `n_estimators`we may not have explored enough since it always maxes out\n",
    "* `min_samples_split`:  given 4 and 8, we can probably cut down some of the larger options in [2,4,8,16,32,64]\n",
    "* `min_samples_leaf`: given 1 and 10, we can probably remove the larger options in [1,2,5,10,30,10%,20%]\n",
    "* `max_features`: this is fairly robustly `None`, but this violates basics of RF... need to explore this more\n",
    "* `max_depth`:  16 is pretty robust (at the least, we can likely leave out \"None\" option from next search)\n",
    "* `criterion`:  just keep allowing search over both...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.531520855249879\n",
      "MAE, Dev: 7.541420337416439 \n",
      "\n",
      "MSE, Trn: 51.29657810071865\n",
      "MSE, Dev: 93.06215468979839 \n",
      "\n",
      "R2, Trn: 0.6353583256984559\n",
      "R2, Dev: 0.31065222392058456\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rfs.predict(x_trn)\n",
    "y_dev_pred = best_rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yikes\n",
    "Ok, wow...  So the GridSearch really is the problem.  It is allowing too much\n",
    "overfitting of the training set.  This is likely due to allowing so many estimators\n",
    "and allowing `max_features` to be the full feature set.\n",
    "\n",
    "Explore this...\n",
    "\n",
    "FIRST:  I'll continue to allow `n_estimators` to grow to 1k, but I am removing the\n",
    "ability for `max_features` to select `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split\n",
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Base RF\n",
    "rf_strat = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [10,100,500,1000],           # 4\n",
    "    'max_features': [2, 3, 'log2','sqrt'],       # 4  (removed None)\n",
    "    'max_depth': [3,4,8,16,32],                  # 5  (removed None)\n",
    "    'min_samples_split': [2,4,8,16],             # 4  (removed 32, 64)\n",
    "    'min_samples_leaf': [1,2,5,10,30],           # 5  (removed 10%, 20%)\n",
    "} # 2*4*4*5*4*5 = 3200-model GridSearch\n",
    "# Let's assume that randomly samplng 250 models is good enough\n",
    "n_iter = 250\n",
    "kfolds = 3\n",
    "\n",
    "# GridSearch\n",
    "best_rfs = RandomizedSearchCV(\n",
    "    rf_strat,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 250 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500,\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 32,\n",
       " 'criterion': 'mse'}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rfs.fit(x_trn,y_trn)\n",
    "best_rfs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 3.3996217273526157\n",
      "MAE, Dev: 8.20697083549055 \n",
      "\n",
      "MSE, Trn: 18.997081555847515\n",
      "MSE, Dev: 109.00891833579237 \n",
      "\n",
      "R2, Trn: 0.8649592646946918\n",
      "R2, Dev: 0.19252830886970007\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rfs.predict(x_trn)\n",
    "y_dev_pred = best_rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YIKES\n",
    "Again, we see MAJOR OVERFITTING.... Serious, disgusting, uh-oh overfitting...\n",
    "\n",
    "|  run                 | n_estimators | min_samples_split | min_samples_leaf | max_features | max_depth | criterion |\n",
    "|----------------------|--------------|-------------------|------------------|--------------|-----------|-----------|\n",
    "| No Strat             | 1000         | 8                 | 1                | None         | 16        | MAE       |\n",
    "| W/ Strat (full hp)   | 1000         | 4                 | 10               | None         | 16        | MSE       | \n",
    "| W/ Strat (reduced hp)| 500          | 4                 | 1                | sqrt         | 32        | MSE       |\n",
    "\n",
    "Interestingly, the overfitting is still huge...yet it selected less estimators.\n",
    "\n",
    "NEXT:\n",
    "* allow estimators to only get up to 100 (add in a few sub-100 possibility, e.g., 25,50,75)\n",
    "* take away option for terminal purity (1/leaf), which is likely noise fitting (maybe even take away 2/leaf)\n",
    "* if min sample per leaf is 4, then adjust `min_samples_split` accordingly (4 is lowest)\n",
    "* use 5 CV folds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 250 candidates, totalling 1250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 out of 1250 | elapsed:  3.5min finished\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 75,\n",
       " 'min_samples_split': 4,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 16,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Split\n",
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Base RF\n",
    "rf_strat = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [10,25,50,75,100],           # 5  (removed 1k, 500; added 25,50,75)\n",
    "    'max_features': [2, 3, 'log2','sqrt'],       # 4  (removed None)\n",
    "    'max_depth': [3,4,8,16,32],                  # 5  (removed None)\n",
    "    'min_samples_split': [4,8,16],               # 3  (removed 32, 64; removed 2)\n",
    "    'min_samples_leaf': [4,8,16,32],             # 4  (removed 10%, 20%; removed 1, 2)\n",
    "} # 2*5*4*5*3*4 = 2400-model GridSearch\n",
    "# Let's assume that randomly samplng 250 models is good enough\n",
    "n_iter = 250\n",
    "kfolds = 5\n",
    "\n",
    "# GridSearch\n",
    "best_rfs = RandomizedSearchCV(\n",
    "    rf_strat,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_rfs.fit(x_trn,y_trn)\n",
    "best_rfs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.645607879347491\n",
      "MAE, Dev: 8.248563218390805 \n",
      "\n",
      "MSE, Trn: 54.87699201805683\n",
      "MSE, Dev: 109.77829042145595 \n",
      "\n",
      "R2, Trn: 0.609906956935664\n",
      "R2, Dev: 0.18682926893238483\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rfs.predict(x_trn)\n",
    "y_dev_pred = best_rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YIKES\n",
    "Wow... It's still so bad.  Grid searches are fucking shitty...  Everyone talks about\n",
    "grid searching, and it seems to work on simple data sets... But here, even w/ 5-fold CV,\n",
    "we can't help but overfit the training set.....\n",
    "\n",
    "We need better regularization yet again....\n",
    "\n",
    "What's weird is:  why did the model work so well on my first try?  That's scary and hopefully\n",
    "I didn't get excited prematurely...\n",
    "\n",
    "|  run                 | n_estimators | min_samples_split | min_samples_leaf | max_features | max_depth | criterion |\n",
    "|----------------------|--------------|-------------------|------------------|--------------|-----------|-----------|\n",
    "| No Strat             | 1000         | 8                 | 1                | None         | 16        | MAE       |\n",
    "| W/ Strat (full hp)   | 1000         | 4                 | 10               | None         | 16        | MSE       | \n",
    "| W/ Strat (reduced hp)| 500          | 4                 | 1                | sqrt         | 32        | MSE       |\n",
    "| W/ Strat (rhp*)      | 75           | 4                 | 4                | sqrt         | 16        | MAE       |\n",
    "\n",
    "NEXT:\n",
    "* further reduce terminal purity \n",
    "* adjust `min_samples_split` accordingly\n",
    "* use 5 CV folds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  4.1min finished\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 75,\n",
       " 'min_samples_split': 16,\n",
       " 'min_samples_leaf': 8,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 32,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Split\n",
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Base RF\n",
    "rf_strat = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [10,25,50,75,100],           # 5  (removed 1k, 500; added 25,50,75)\n",
    "    'max_features': [2, 3, 'log2','sqrt'],       # 4  (removed None)\n",
    "    'max_depth': [3,4,8,16,32],                  # 5  (removed None)\n",
    "    'min_samples_split': [8,16],                 # 2  (removed 32, 64; rem 2; rem 4)\n",
    "    'min_samples_leaf': [8,16,32],               # 3  (removed 10%, 20%; rem 1, 2; rem 4)\n",
    "} # 2*5*4*5*2*3 = 1200-model GridSearch\n",
    "# Let's assume that randomly samplng 250 models is good enough\n",
    "n_iter = 300\n",
    "kfolds = 5\n",
    "\n",
    "# GridSearch\n",
    "best_rfs = RandomizedSearchCV(\n",
    "    rf_strat,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_rfs.fit(x_trn,y_trn)\n",
    "best_rfs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 6.628224068944291\n",
      "MAE, Dev: 8.181379310344827 \n",
      "\n",
      "MSE, Trn: 75.03606824664\n",
      "MSE, Dev: 107.01126896551723 \n",
      "\n",
      "R2, Trn: 0.46660618365737705\n",
      "R2, Dev: 0.20732567902920018\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rfs.predict(x_trn)\n",
    "y_dev_pred = best_rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geez\n",
    "Now we're running into a new problem: the model is sucking on the training set too....so we\n",
    "are underfitting.......\n",
    "\n",
    "|  run                 | n_estimators | min_samples_split | min_samples_leaf | max_features | max_depth | criterion |\n",
    "|----------------------|--------------|-------------------|------------------|--------------|-----------|-----------|\n",
    "| No Strat             | 1000         | 8                 | 1                | None         | 16        | MAE       |\n",
    "| W/ Strat (full hp)   | 1000         | 4                 | 10               | None         | 16        | MSE       | \n",
    "| W/ Strat (reduced hp)| 500          | 4                 | 1                | sqrt         | 32        | MSE       |\n",
    "| W/ Strat (rhp*)      | 75           | 4                 | 4                | sqrt         | 16        | MAE       |\n",
    "| W/ Strat (rhp**)     | 75           | 16                | 8                | sqrt         | 32        | MAE       |\n",
    "\n",
    "NEXT:\n",
    "* maybe we should allow depth to grow again?\n",
    "    - but that doesn't make too much sense: no matter the model, it always seems to\n",
    "      cap depth at 16 or 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 400 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 14.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed: 16.1min finished\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 300,\n",
       " 'min_samples_split': 8,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 16,\n",
       " 'criterion': 'mae'}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Split\n",
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "# Base RF\n",
    "rf_strat = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {\n",
    "    'criterion': ['mse','mae'],                  # 2\n",
    "    'n_estimators': [75,150,300,600],            # 4  (removed 1k, 500; added 25,50,75)\n",
    "    'max_features': [2, 3, 'log2','sqrt'],       # 4  (removed None)\n",
    "    'max_depth': [3,4,8,16,32],                  # 5  (removed None)\n",
    "    'min_samples_split': [8,16],                 # 2  (removed 32, 64; rem 2; rem 4)\n",
    "    'min_samples_leaf': [4,8,16,32],             # 4  (removed 10%, 20%; rem 1, 2; rem 4)\n",
    "} # 2*5*4*5*2*4 = 1600-model GridSearch\n",
    "# Let's assume that randomly samplng 400 models is good enough\n",
    "n_iter = 400\n",
    "kfolds = 5\n",
    "\n",
    "# GridSearch\n",
    "best_rfs = RandomizedSearchCV(\n",
    "    rf_strat,\n",
    "    grid,\n",
    "    n_jobs  = -1,\n",
    "    cv      = kfolds,\n",
    "    n_iter  = n_iter,  # e.g., 250 models searched\n",
    "    return_train_score = True,\n",
    "    scoring = ['r2', 'neg_mean_absolute_error',\n",
    "               'neg_mean_squared_error','neg_median_absolute_error'],\n",
    "    refit   = 'neg_mean_squared_error',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_rfs.fit(x_trn,y_trn)\n",
    "best_rfs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.634903047091413\n",
      "MAE, Dev: 8.206163793103448 \n",
      "\n",
      "MSE, Trn: 54.4257994716323\n",
      "MSE, Dev: 107.79785174808431 \n",
      "\n",
      "R2, Trn: 0.6131142587022183\n",
      "R2, Dev: 0.20149915272887575\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = best_rfs.predict(x_trn)\n",
    "y_dev_pred = best_rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=16,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=4, min_samples_split=8,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfz = RandomForestRegressor(\n",
    "    n_estimators = 300,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 4,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 16,\n",
    "    criterion = 'mae',\n",
    "    random_state= 42\n",
    ")\n",
    "rfz.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.634903047091413\n",
      "MAE, Dev: 8.206163793103448 \n",
      "\n",
      "MSE, Trn: 54.4257994716323\n",
      "MSE, Dev: 107.79785174808431 \n",
      "\n",
      "R2, Trn: 0.6131142587022183\n",
      "R2, Dev: 0.20149915272887575\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = rfz.predict(x_trn)\n",
    "y_dev_pred = rfz.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mae', max_depth=16,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=4, min_samples_split=8,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW W/O RS42\n",
    "rfz = RandomForestRegressor(\n",
    "    n_estimators = 300,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 4,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 16,\n",
    "    criterion = 'mae',\n",
    ")\n",
    "rfz.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.674893813481071\n",
      "MAE, Dev: 8.275495689655171 \n",
      "\n",
      "MSE, Trn: 55.147870236996\n",
      "MSE, Dev: 109.3792294420498 \n",
      "\n",
      "R2, Trn: 0.6079814193863198\n",
      "R2, Dev: 0.1897852696782375\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = rfz.predict(x_trn)\n",
    "y_dev_pred = rfz.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=16,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=4, min_samples_split=8,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW W/O RS42 & criterion->MSE\n",
    "rfz = RandomForestRegressor(\n",
    "    n_estimators = 300,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 4,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 16,\n",
    "    criterion = 'mse',\n",
    ")\n",
    "rfz.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 5.233870293948408\n",
      "MAE, Dev: 8.263820484164306 \n",
      "\n",
      "MSE, Trn: 45.085618862364385\n",
      "MSE, Dev: 109.97873962191274 \n",
      "\n",
      "R2, Trn: 0.6795089232538202\n",
      "R2, Dev: 0.18534446330960108\n"
     ]
    }
   ],
   "source": [
    "y_trn_pred = rfz.predict(x_trn)\n",
    "y_dev_pred = rfz.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question:  can I even replicate the good results from before?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 3.7923079409048936\n",
      "MAE, Dev: 7.530265086206896 \n",
      "\n",
      "MSE, Trn: 24.189438289704523\n",
      "MSE, Dev: 93.50718946443965 \n",
      "\n",
      "R2, Trn: 0.8280494020273028\n",
      "R2, Dev: 0.30735567729323054\n"
     ]
    }
   ],
   "source": [
    "bins = np.linspace(0, 61, 10)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=42)\n",
    "x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=42)\n",
    "\n",
    "rfs = RandomForestRegressor(\n",
    "    n_estimators = 1000,\n",
    "    min_samples_split = 8,\n",
    "    min_samples_leaf = 1,\n",
    "    max_features = None,\n",
    "    max_depth = 16,\n",
    "    criterion = 'mae',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# Crazy: note that I didn't use random state 42 here...and got great results...\n",
    "\n",
    "rfs.fit(x_trn, y_trn)\n",
    "\n",
    "y_trn_pred = rfs.predict(x_trn)\n",
    "y_dev_pred = rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Trn: 6.332746352004538\n",
      "MAE, Dev: 8.513622287019183 \n",
      "\n",
      "MSE, Trn: 61.14664143182258\n",
      "MSE, Dev: 113.6974537476489 \n",
      "\n",
      "R2, Trn: 0.5704851040440035\n",
      "R2, Dev: 0.201128653903908\n"
     ]
    }
   ],
   "source": [
    "bins = np.linspace(0, 61, 20)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=37)\n",
    "#x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, random_state=37)\n",
    "\n",
    "#x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=23)\n",
    "\n",
    "# I changed max_features to sqrt here\n",
    "rfs = RandomForestRegressor(\n",
    "    n_estimators = 5000,\n",
    "    #min_samples_split = 16,\n",
    "    min_samples_leaf = 1,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 8,\n",
    "    criterion = 'mse',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# Crazy: note that I didn't use random state 42 here...and got great results...\n",
    "\n",
    "rfs.fit(x_trn, y_trn)\n",
    "\n",
    "y_trn_pred = rfs.predict(x_trn)\n",
    "y_dev_pred = rfs.predict(x_dev)\n",
    "\n",
    "print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=5000, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = np.linspace(0, 61, 20)\n",
    "yb = np.digitize(y, bins)\n",
    "x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, stratify=yb, random_state=37)\n",
    "#x_trn, x_dev, y_trn, y_dev = train_test_split(x, y, test_size=0.3, random_state=37)\n",
    "\n",
    "#x_dev, x_tst, y_dev, y_tst = train_test_split(x_dev, y_dev, test_size=0.5, random_state=23)\n",
    "\n",
    "# I changed max_features to sqrt here\n",
    "rfs = RandomForestRegressor(\n",
    "    n_estimators = 5000,\n",
    "    #min_samples_split = 16,\n",
    "    min_samples_leaf = 1,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 8,\n",
    "    criterion = 'mse',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# Crazy: note that I didn't use random state 42 here...and got great results...\n",
    "\n",
    "rfs.fit(x_trn, y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn_rfs_pred = rfs.predict(x_trn)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(y_trn_rfs_pred.reshape(-1,1), y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Dev: 8.132628395412109 \n",
      "\n",
      "MSE, Dev: 104.82073340700845 \n",
      "\n",
      "R2, Dev: 0.26349907024748986\n"
     ]
    }
   ],
   "source": [
    "y_dev_pred = lr.predict(rfs.predict(x_dev).reshape(-1,1))\n",
    "\n",
    "#print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "#print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "#print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfs2 = RandomForestRegressor(\n",
    "    n_estimators = 100,\n",
    "    #min_samples_split = 16,\n",
    "    min_samples_leaf = 1,\n",
    "    max_features = 'sqrt',\n",
    "    max_depth = 8,\n",
    "    criterion = 'mse',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rfs2.fit(y_trn_rfs_pred.reshape(-1,1), y_trn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE, Dev: 8.128229188468467 \n",
      "\n",
      "MSE, Dev: 108.83975909684959 \n",
      "\n",
      "R2, Dev: 0.23526022797786195\n"
     ]
    }
   ],
   "source": [
    "y_dev_pred = rfs2.predict(rfs.predict(x_dev).reshape(-1,1))\n",
    "\n",
    "#print('MAE, Trn:', mean_absolute_error(y_trn, y_trn_pred))\n",
    "print('MAE, Dev:', mean_absolute_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "#print('MSE, Trn:', mean_squared_error(y_trn, y_trn_pred))\n",
    "print('MSE, Dev:', mean_squared_error(y_dev, y_dev_pred),'\\n')\n",
    "\n",
    "#print('R2, Trn:', r2_score(y_trn, y_trn_pred))\n",
    "print('R2, Dev:', r2_score(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
