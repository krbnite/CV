{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_score, average_precision_score, \\\n",
    "    accuracy_score, balanced_accuracy_score, recall_score, confusion_matrix\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from matplotlib import pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9989231873653984\n",
      "Val Accuracy: 0.9916666666666667\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9989231873653984\n",
      "Val Bal Accuracy: 0.6641541038525963\n",
      "-------------------\n",
      "Trn AUROC: 0.9989231873653984\n",
      "Val AUROC: 0.6641541038525963\n",
      "-------------------\n",
      "Trn AUPRC: 0.9978510028653295\n",
      "Val AUPRC: 0.08666666666666667\n",
      "-------------------\n",
      "Trn Precision Score: 0.9989255014326648\n",
      "Val Precision Score: 0.9929110738255033\n",
      "-------------------\n",
      "Trn Recall Score: 0.9989231873653984\n",
      "Val Recall Score: 0.9916666666666667\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1390    3]\n",
      " [   0 1393]]\n",
      "Val Confusion:\n",
      " [[594   3]\n",
      " [  2   1]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'Outcome'\n",
    "data = pd.read_csv('../data/external/synthetic-data-sanity-check.csv')\n",
    "data.drop([col for col in data.columns if 'Unnamed' in col], axis=1, inplace=True)\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(['ID','Outcome'], axis=1)\n",
    "y = (data[[target]].copy() + 1) % 2  # Dani defined 1 as majority class, 0 as event class\n",
    "\n",
    "\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "# -- NOTE: Dani's synthetic data does not have any missing y values,\n",
    "#      so this step is technically unnecessary; left here to be consistent\n",
    "#      with SMOTE/MIM/L1-LogReg code from previous notebook\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "# -- NOTE: Dani has missing values in EVERY column, whereas real data set\n",
    "#      only has missing data in most cols\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "    \n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=23)\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "# -- NOTE:  Dani's original synthetic data set has no categoricals/ordinals,\n",
    "#      so this code is not technically needed here, however I've emailed her\n",
    "#      to generate a second synthetic data set w/ such var types in it; keeping\n",
    "#      this here for consistency\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= 10]\n",
    "for col in nots:\n",
    "    min_trn = x_trn[col].min()\n",
    "    max_trn = x_trn[col].max()\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    ##### NOTE2SELF:  when using SAGA solver, you have to scale...which I\n",
    "    #####   did in project, but forgot to apply to x_val\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "    \n",
    "# SMOTE the training data\n",
    "# -- NOTE: since no categoricals in Dani's original data set, it might be\n",
    "#      argued that we should be using regular SMOTE here (not SMOTENC)\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "#sm = SMOTENC(cats, random_state=37)\n",
    "sm = SMOTE(random_state=37)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', tol=0.01, C=0.5)#, l1_ratio=1)#'liblinear')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "#=======================================\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFs\n",
    "These do amazing!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9813352476669059\n",
      "Val Accuracy: 0.9616666666666667\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9813352476669059\n",
      "Val Bal Accuracy: 0.9807370184254607\n",
      "-------------------\n",
      "Trn AUROC: 0.9813352476669059\n",
      "Val AUROC: 0.9807370184254606\n",
      "-------------------\n",
      "Trn AUPRC: 0.9640138408304498\n",
      "Val AUPRC: 0.11538461538461539\n",
      "-------------------\n",
      "Trn Precision Score: 0.982006920415225\n",
      "Val Precision Score: 0.995576923076923\n",
      "-------------------\n",
      "Trn Recall Score: 0.9813352476669059\n",
      "Val Recall Score: 0.9616666666666667\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1341   52]\n",
      " [   0 1393]]\n",
      "Val Confusion:\n",
      " [[574  23]\n",
      " [  0   3]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'Outcome'\n",
    "data = pd.read_csv('../data/external/synthetic-data-sanity-check.csv')\n",
    "data.drop([col for col in data.columns if 'Unnamed' in col], axis=1, inplace=True)\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(['ID','Outcome'], axis=1)\n",
    "y = (data[[target]].copy() + 1) % 2  # Dani defined 1 as majority class, 0 as event class\n",
    "\n",
    "\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "# -- NOTE: Dani's synthetic data does not have any missing y values,\n",
    "#      so this step is technically unnecessary; left here to be consistent\n",
    "#      with SMOTE/MIM/L1-LogReg code from previous notebook\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "# -- NOTE: Dani has missing values in EVERY column, whereas real data set\n",
    "#      only has missing data in most cols\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "    \n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=23)\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "# -- NOTE:  Dani's original synthetic data set has no categoricals/ordinals,\n",
    "#      so this code is not technically needed here, however I've emailed her\n",
    "#      to generate a second synthetic data set w/ such var types in it; keeping\n",
    "#      this here for consistency\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= 10]\n",
    "for col in nots:\n",
    "    min_trn = x_trn[col].min()\n",
    "    max_trn = x_trn[col].max()\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    ##### NOTE2SELF:  when using SAGA solver, you have to scale...which I\n",
    "    #####   did in project, but forgot to apply to x_val\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "    \n",
    "# SMOTE the training data\n",
    "# -- NOTE: since no categoricals in Dani's original data set, it might be\n",
    "#      argued that we should be using regular SMOTE here (not SMOTENC)\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "#sm = SMOTENC(cats, random_state=37)\n",
    "sm = SMOTE(random_state=37)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "model = RandomForestClassifier(n_estimators=10000, max_depth=1, n_jobs=-1)\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "#=======================================\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
