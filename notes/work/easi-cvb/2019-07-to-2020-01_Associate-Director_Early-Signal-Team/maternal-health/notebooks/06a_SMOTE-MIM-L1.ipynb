{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "It's time to pull out some tricks.  \n",
    "\n",
    "The random forest models are still \n",
    "* not performing very well\n",
    "* and are fairly sensitive to random seed\n",
    "\n",
    "As I've hypothesized earlier, this seems to suggest that \n",
    "* there is an important independent variable (or several) that dictate the performance\n",
    "* however, this is a very rare event problem, so there are very few events to learn from\n",
    "* this means that an important independent variable can be improperly split into training,\n",
    "  validation, and test sets -- that is, without stratifying on these variables, the training\n",
    "  set does not have a representative distribution of those independent variables\n",
    "* some models can recover from this (e.g., a linear model that learns a formula)\n",
    "* unfortunately, this is one of the few weakness of random forests, where a major assumption is\n",
    "  that the training set is a representative sample of the population at large (especially when\n",
    "  it comes to extrema -- mins and maxes)\n",
    "\n",
    "# SMOTE\n",
    "The imbalanced class (\"rare event\") problem should have not been overlooked.  However, this\n",
    "project has changed hands a few times and we have always been on a tight deadline.  At WWE,\n",
    "I had success in improving machine learning classifiers by using SMOTE (\"Synthetic Minority\n",
    "Over-sampling TEchnique\").  This technique is applied to the training set to grow the \n",
    "minority class to a similar size as the majority class.  A simpler version of this technique\n",
    "is to simply oversample the minority: duplicate the minority class records over and over until\n",
    "you have similarly sized classes.  This can over-emphasize the particular minority examples in\n",
    "the training set, which SMOTE hedges against a bit by generating synthetic records that look\n",
    "like minority class records, but not perfectly.\n",
    "\n",
    "Note that the empahsis on applying this technique to the training set:  there is risk for \n",
    "data leakage when using any data augmentation technique.  Namely, if you use SMOTE on the whole data\n",
    "set before splitting, you are doing it wrong!  This will directly cause leakage (\"information pathways\")\n",
    "between the training, validation, and test sets.\n",
    "\n",
    "Python has a great module dedicated to the imbalanced class problem:\n",
    "* https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "\n",
    "\n",
    "You can get your hands on SMOTE quite easily:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "```\n",
    "\n",
    "If you have a lot of categorical variables, it's actually better to use a modified version\n",
    "of SMOTE for this use case:\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "```\n",
    "\n",
    "For more info, read the original paper on SMOTE:\n",
    "* N. V. Chawla, K. W. Bowyer, L. O.Hall, W. P. Kegelmeyer, “SMOTE: synthetic minority over-sampling technique,” Journal of artificial intelligence research, 321-357, 2002.\n",
    "\n",
    "\n",
    "# Parametric Learning\n",
    "In addition to class imblance, another issue I identified above is tied to \n",
    "the dependence of the random forest's performance on random seed.  Though there will\n",
    "always be some variability when switching seeds, it shouldn't be the difference\n",
    "between a 0.5 AUC and a 0.6 AUC (this is shown below with random forests).  \n",
    "\n",
    "A significant dependence on random seed\n",
    "implies that the model is sensitively dependent on the specifics and details of\n",
    "the training set.  Specifically,\n",
    "this implies a dependence on how certain important variables are distributed into\n",
    "training, validation, and test.  \n",
    "\n",
    "Some models are robust against this.  For example, assume the target is linearly \n",
    "dependent on an important independent variable, and assume that the independent \n",
    "variable was poorly split into training and validation, where the training set\n",
    "sees only the bottom 70% of the variables range, while the training set only\n",
    "holds instances in the range's top 30%.  A linear regression will learn a linear\n",
    "formula on the training set, which will still be largely applicable and useful\n",
    "on the validation set -- even though it never saw instances in the range's top\n",
    "30%.  However, a random forest will not be able to do this.  Despite all of the\n",
    "great properties of a random forest, it cannot extrapolate.  \n",
    "\n",
    "In a rare event\n",
    "classification problem, this issue is more likely to affect a random forest and\n",
    "can only be mitigated if you properly stratify the data split on the important \n",
    "variables. Note that without this stratification, a signal augmentation technique\n",
    "like SMOTE will not likely help all that much.\n",
    "\n",
    "\n",
    "In previous attempts, we tried to mitigate this issue by stratifying on the target variable,\n",
    "which helped ensure that the training distribution of the target matched the\n",
    "distribution in validation; but this was not enough!  The poor performance implied\n",
    "that there might be no signal, but the dependence on random seed implied that\n",
    "there was -- some important variable(s) exist that, when split correctly (\"when\n",
    "using the right random seed\"), improves the model performance.  However, it's not\n",
    "clear how to identify all the right variables to stratify on.  Instead, given my\n",
    "hypothesis, it would be better to use a parametric model, which has a chance of \n",
    "learning a formula that can extrapolate.  \n",
    "\n",
    "Why not try logistic regression?\n",
    "\n",
    "# Missing Indicator Method\n",
    "One of the great properties of a random forest is that you really do not have to\n",
    "do much to the data.  For example, it will figure out a linear, quadratic, or\n",
    "exponential dependence equally well (or any monotonic transformation).  It is\n",
    "also reasonably well-suited for figuring out conditional relationships (after all,\n",
    "each tree is basically a heirarchy of if-then statements), which means you do not\n",
    "necessarily have to worry about adding interactions in explicitly (e.g., x1, x2, and\n",
    "x1\\*x2).  If one assumes that missing data is validly informative (i.e., does not introduce\n",
    "data leakage), then it can basically be left as-is or coded as some\n",
    "extreme value way outside the variable's range (e.g.,-999999), whereas this would\n",
    "destroy a linear model which requires a well-conceived imputation strategy.  \n",
    "\n",
    "The missing indicator method (MIM) is a technique that is considered terrible\n",
    "by statisticians interested in unbiased estimates of model coefficients (relationship\n",
    "strengths), but which is actually top notch when it comes to building a predictive\n",
    "model where one is less interested in unbiased coefficients and more interested\n",
    "on predictive performance on held out data sets.  \n",
    "\n",
    "To implement the MIM, you simply take any column with missing data (call it x) and\n",
    "make a second column (call it x\\_missing).  Wherever x has a missing value, x\\_missing\n",
    "is flagged with a 1, but is 0 otherwise.  Then, the missing values in x are recoded to\n",
    "0.  \n",
    "\n",
    "So, say you have a single-variable modes like so:\n",
    "```\n",
    "y = m*x+b\n",
    "```\n",
    "\n",
    "This now becomes:\n",
    "```\n",
    "y = m1*x + m2*x_missing + b\n",
    "```\n",
    "\n",
    "As you can see, instead of imputing with the median or mean, this technique will\n",
    "optimize the best replacement value (i.e., after learning has occurred, you can \n",
    "essentially rewrite `m2` as `m2 = m1*x_replacement`; this allows `m1` to \n",
    "be learned without forcing an imputed value on it).\n",
    "\n",
    "Since random forests are fairly robust against variable representation, its performance\n",
    "shouldn't be affected all that much by choosing to use the MIM (or not).  However, predictive models\n",
    "that require an imputation strategy will often show incredible performance boosts using\n",
    "this technique.\n",
    "\n",
    "\n",
    "# L1 Regularization\n",
    "The downside to the MIM is that it can double the data dimensionality if every\n",
    "variable has missing data (which is nearly true for our data set).  This wouldn't be\n",
    "so bad if every variable was important -- but you can basically assume beforehand that\n",
    "every variable is not important, and many are probably no better than noise.  This means\n",
    "that you might be doubling the amount of useless variable, which can cause degradation in\n",
    "model performance.  \n",
    "\n",
    "Fortunately, there are techniques for eliminating variables in regressions.  One such technique\n",
    "in linear regression is called LASSO, which uses the L1 penalty and chooses to zero out variables\n",
    "that a penalized below a certain threshold.  Importantly, the L1 penalty does most of the\n",
    "hard work here:  it serves to amplify coefficients of variables with signal and diminish the \n",
    "coefficients of noisy, useless variables.  \n",
    "\n",
    "# Results\n",
    "In this notebook, I explore:\n",
    "* SMOTE, SMOTENC\n",
    "* MIM\n",
    "* RandomForest -> Logistic Regression\n",
    "* L1 Penalized Logistic Regression \n",
    "    - and for comparison: no penalty, L2 penalty, and ElasticNet penalty\n",
    "\n",
    "\n",
    "I find that SMOTE can help a random forest, but as assumed, only when the data\n",
    "split was opportunistic.  For certain \"bad\" data splits (based on random seed), SMOTE\n",
    "cannot help the random forest.\n",
    "\n",
    "I then move on logistic regression, where I automatically include MIM because logistic\n",
    "regression requires putting some thought into an imputation strategy.  **Starting with\n",
    "SMOTE and L1 penalty, we find the AUC on validation at 0.995.**  Remember: the highest\n",
    "AUC we got with \"opportunistic\" splits using the RF was around 0.58.  \n",
    "\n",
    "I then investigate the cause of such a dramatic increase in AUC.  I find that SMOTE\n",
    "is absolutely necessary for this success (without it, AUC suffers significantly).  I also\n",
    "find that logistic regression performs well in general, independent of solver used: the worst\n",
    "seen with no penalty was around 0.66 AUC, though most of the solvers got up to 0.83 AUC with\n",
    "no penalty; L2 penalty has no improvement over 0.83 AUC (and in fact, almost the entire\n",
    "tradeoff range in ElasticNet performs the same as well); however, nothing comes close\n",
    "to beating pure L1 penalty.  Finally, by playing with random seeds a bit, I find that\n",
    "the results are largely independent of random seed.  \n",
    "\n",
    "\n",
    "# Next Steps\n",
    "In the next notebook, I will explore variable importances in more detail.  I will\n",
    "also look into using LIME to understand model decisions at the single-decision level. Finally,\n",
    "since we found that the sensor data is nearly 100% ineffective (quick glance at importances in\n",
    "this notebook places them at the tail end), I will use the larger \"democlinical patient set\"\n",
    "to build this model on.  This set is about twice the size of the Toitu-restricted set I've been\n",
    "using.  Importantly, since these ~1500 patients are unseen in this notebook, I should have\n",
    "a holdout sample from this subset that is tested last...  \n",
    "\n",
    "Since I do not have a test set in this notebook, these results should be strengthened using\n",
    "cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_score, average_precision_score, \\\n",
    "    accuracy_score, balanced_accuracy_score, recall_score, confusion_matrix\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "target_cols = [\n",
    "    'adscmat_labortype_spontaneousoraugmented_db', \n",
    "    'adsc_dlvry_dt', \n",
    "    'adsc_dlvrybefore28wks', \n",
    "    'adsc_dlvrybefore34wks', \n",
    "    'adsc_dlvryga_dys', \n",
    "    'adsc_dlvryga_dys_sbadj']\n",
    "\n",
    "# Get Full Dataset\n",
    "dff = pd.read_csv('../data/processed/full_set_v4_20200110_KU.csv')\n",
    "\n",
    "# Set PatId Index\n",
    "dff.set_index('patid', inplace=True)\n",
    "\n",
    "# Remove Cesareans/Inductions\n",
    "dff.query('adscmat_labortype_spontaneousoraugmented_db == 1', inplace=True)\n",
    "\n",
    "# Remove records w/ GA@Birth < 25 wks (168 days is last day of week 24, so let's do 170 for clean cut off)\n",
    "dff.query('adsc_dlvryga_dys >= 170', inplace=True)\n",
    "\n",
    "# NEW: Remove records w/ GA@Recruitment > 25 weeks\n",
    "dff.query('adelig_ccga <= 170', inplace=True)\n",
    "dff.drop('adelig_ccga', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Feature Subsets\n",
    "#  -- Sensor Cols:       Monica and Toitu Cols (usually; we dropped Monica above)\n",
    "#  -- Toitu Cols:        Toitu Cols\n",
    "#  -- Toitu F1 Cols:     Toitu F1 Cols\n",
    "#  -- Toitu F1 Pwr Cols: Toitu F1 Power Cols (band21, band31, band41)\n",
    "democlinical_cols = [col for col in dff.columns if 'sensor' not in col]\n",
    "sensor_cols = [col for col in dff.columns if 'sensor' in col]\n",
    "monica_cols = [col for col in sensor_cols if 'mon' in col]\n",
    "toitu_cols = [col for col in sensor_cols if 'toi' in col]\n",
    "toitu_f1_cols = [col for col in toitu_cols if 'f1' in col]\n",
    "toitu_f1_pwr_cols = [col for col in toitu_f1_cols if 'pwr' in col]\n",
    "toitu_f1_orthog_pwr_cols = [col for col in toitu_f1_pwr_cols if '41' not in col]\n",
    "toitu_f3_cols = [col for col in toitu_cols if 'f3' in col]\n",
    "toitu_f3_pwr_cols = [col for col in toitu_f3_cols if 'pwr' in col]\n",
    "\n",
    "\n",
    "# NOTE: We are simply dropping all Monica Cols for this analysis\n",
    "#rmssd_data_quality = [col for col in monica_cols \n",
    "#                     if 'rwaves' in col or 'perc' in col or 'epoch' in col]\n",
    "#dff.drop(rmssd_data_quality, axis=1, inplace=True)\n",
    "dff.drop(monica_cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Patient Subsets\n",
    "#  -- Toitu F1 (906): patients with all 8 Toitu F1 vars\n",
    "#  -- Toitu F1 Pwr (1853):  patients with all 3 Toitu F1 Power vars\n",
    "#  -- Toitu F1 Orthog Power (1853):  patients with power bands 21 and 31\n",
    "#  -- Toitu F1F3 (389): patients with all F1 and F3 vars\n",
    "#  -- Toitu F1F3 Pwr (389):  patients with all F1 and F3 Power vars\n",
    "toitu_f1_population = \\\n",
    "    dff[toitu_f1_cols].replace(-999999,np.nan).isnull().sum(axis=1).map(lambda x: x==0)\n",
    "toitu_f1_pwr_population = \\\n",
    "    dff[toitu_f1_pwr_cols].replace(-999999,np.nan).isnull().sum(axis=1).map(lambda x: x==0)\n",
    "toitu_f1_orthog_pwr_population = \\\n",
    "    dff[toitu_f1_orthog_pwr_cols].replace(-999999,np.nan).isnull().sum(axis=1).map(lambda x: x==0)\n",
    "toitu_f1f3_population = \\\n",
    "    dff[toitu_f1_cols+toitu_f3_cols].replace(-999999,np.nan).isnull().sum(axis=1).map(lambda x: x==0)\n",
    "toitu_f1f3_pwr_population = \\\n",
    "    dff[toitu_f1_cols+toitu_f3_cols].replace(-999999,np.nan).isnull().sum(axis=1).map(lambda x: x==0)\n",
    "\n",
    "\n",
    "# Data Subsets (rows_cols_df)\n",
    "##1\n",
    "toif1_demo_df = dff.copy().loc[toitu_f1_pwr_population, democlinical_cols]\n",
    "##2\n",
    "toif1pwr_toif1pwr_df = dff.copy().loc[toitu_f1_pwr_population, target_cols + toitu_f1_pwr_cols]\n",
    "##3\n",
    "toif1pwr_demotoif1pwr_df = dff.copy().\\\n",
    "    loc[toitu_f1_pwr_population, democlinical_cols + toitu_f1_pwr_cols]\n",
    "\n",
    "\n",
    "### EXTRAS: Only worth checking out if 3 pwr cols do well\n",
    "toif1orthog_toif1orthog_df = dff.copy().\\\n",
    "    loc[toitu_f1_orthog_pwr_population, target_cols + toitu_f1_orthog_pwr_cols]\n",
    "toif1orthog_demotoif1orthog_df = dff.copy().\\\n",
    "    loc[toitu_f1_orthog_pwr_population, democlinical_cols + toitu_f1_orthog_pwr_cols]\n",
    "\n",
    "target_cols = [\n",
    "    'adscmat_labortype_spontaneousoraugmented_db', \n",
    "    'adsc_dlvry_dt', \n",
    "    'adsc_dlvrybefore28wks', \n",
    "    'adsc_dlvrybefore34wks', \n",
    "    'adsc_dlvryga_dys', \n",
    "    'adsc_dlvryga_dys_sbadj'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE\n",
    "Here, we are still using a RandomForest, but we will be introducing SMOTE.\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 100 trees\n",
    "* Missing Data\n",
    "    - Not treated (left as -999999)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using regular SMOTE\n",
    "* Results\n",
    "    - Terrible: ~0.5 AUC\n",
    "    - **UPDATE (2020-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have a 0.58 AUC\n",
    "      here...  except that on scenarios below that had higher AUCs, we now have lower AUCs;\n",
    "      it's nearly inexplicable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.9730215827338129\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.5873076503328604\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.5873076503328604\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.1575327972915785\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.9677692628505891\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.9730215827338129\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1258    0]\n",
      " [   0 1258]]\n",
      "Val Confusion:\n",
      " [[538   1]\n",
      " [ 14   3]]\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcsmk_t1_avecigs.wk</td>\n",
       "      <td>0.064474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_avecigs.wk</td>\n",
       "      <td>0.055850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admh_hxnt2</td>\n",
       "      <td>0.050961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.048630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.047970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>admh_hxpt2</td>\n",
       "      <td>0.046879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adscmat_empl</td>\n",
       "      <td>0.034887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>advs_prepregweight</td>\n",
       "      <td>0.029889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adafppappa_afpmom</td>\n",
       "      <td>0.028739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>0.027622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x_col  gini_importance\n",
       "1    alcsmk_t1_avecigs.wk         0.064474\n",
       "2    alcsmk_t2_avecigs.wk         0.055850\n",
       "3              admh_hxnt2         0.050961\n",
       "4            adsc_gender2         0.048630\n",
       "5   adscmat_educ_combohs4         0.047970\n",
       "6              admh_hxpt2         0.046879\n",
       "7            adscmat_empl         0.034887\n",
       "8      advs_prepregweight         0.029889\n",
       "9       adafppappa_afpmom         0.028739\n",
       "10           admh_anemia3         0.027622"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTE(random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTENC\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 100 trees\n",
    "* Missing Data\n",
    "    - Not treated (left as -999999)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using SMOTENC\n",
    "* Result\n",
    "    - showed modest improvement ~0.5 AUC -> 0.53\n",
    "    - so it's likely SMOTENC we want, but note that we can do better by properly treating\n",
    "      missing data (see next section)\n",
    "    - **UPDATE (2019-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have entirely different\n",
    "      results (e.g., SMOTENC degrades the AUC of SMOTE for random seed 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.9676258992805755\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.5560405980574048\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.5560405980574048\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.07403724079559881\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.9552638107300002\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.9676258992805755\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1258    0]\n",
      " [   0 1258]]\n",
      "Val Confusion:\n",
      " [[536   3]\n",
      " [ 15   2]]\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.100135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_avecigs.wk</td>\n",
       "      <td>0.058787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alcsmk_t1_avecigs.wk</td>\n",
       "      <td>0.048279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>0.047013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>0.045731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>advs_prepregbmi</td>\n",
       "      <td>0.033973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adsmk_avgnumcighome</td>\n",
       "      <td>0.031529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adstai_s_anxiety</td>\n",
       "      <td>0.029696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>advs_prepregweight</td>\n",
       "      <td>0.028763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.028331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x_col  gini_importance\n",
       "1   adscmat_educ_combohs4         0.100135\n",
       "2    alcsmk_t2_avecigs.wk         0.058787\n",
       "3    alcsmk_t1_avecigs.wk         0.048279\n",
       "4            admh_anemia3         0.047013\n",
       "5    adscmat_toiletwater2         0.045731\n",
       "6         advs_prepregbmi         0.033973\n",
       "7     adsmk_avgnumcighome         0.031529\n",
       "8        adstai_s_anxiety         0.029696\n",
       "9      advs_prepregweight         0.028763\n",
       "10           adsc_gender2         0.028331"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Indicator Method\n",
    "Note that, though a random forest can handle missing values be coded \n",
    "as -999999, most other methods cannot -- and this basically includes SMOTE.  Why?\n",
    "Because SMOTE is using nearest neighbor methods, and you can bet those -999999 are\n",
    "throwing things off a bit.  \n",
    "\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 100 trees\n",
    "* Missing Data\n",
    "    - Missing Indictor Method (for every colX w/ missing data, another column is\n",
    "      created, colX_missing, which flags all missing data in colX as 1 and non-missing\n",
    "      data as 0; the missing data in colX are then zeroed out)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using SMOTENC\n",
    "* Result\n",
    "    - as would be expected by a RF, the typical 100-trees run gave back\n",
    "      an AUC of 0.53 (RFs are usually fairly robust against these types of\n",
    "      representational changes)\n",
    "    - **UPDATE (2020-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have entirely different\n",
    "      results (e.g., MIM degrades the AUC instead of maintaining it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.9694244604316546\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.5\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.5\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.030575539568345324\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.9397837844832048\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.9694244604316546\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1258    0]\n",
      " [   0 1258]]\n",
      "Val Confusion:\n",
      " [[539   0]\n",
      " [ 17   0]]\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.087860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>admh_hxft2</td>\n",
       "      <td>0.057540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.043230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>0.029335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>advs_prepregweight</td>\n",
       "      <td>0.028478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>advs_prepregbmi</td>\n",
       "      <td>0.026149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adstai_s_anxiety</td>\n",
       "      <td>0.025883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adstai_t_anxiety</td>\n",
       "      <td>0.023718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adscmat_grossincome7</td>\n",
       "      <td>0.022637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adsmk_avgnumcighome</td>\n",
       "      <td>0.022536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    x_col  gini_importance\n",
       "1   adscmat_educ_combohs4         0.087860\n",
       "2              admh_hxft2         0.057540\n",
       "3            adsc_gender2         0.043230\n",
       "4    adscmat_toiletwater2         0.029335\n",
       "5      advs_prepregweight         0.028478\n",
       "6         advs_prepregbmi         0.026149\n",
       "7        adstai_s_anxiety         0.025883\n",
       "8        adstai_t_anxiety         0.023718\n",
       "9    adscmat_grossincome7         0.022637\n",
       "10    adsmk_avgnumcighome         0.022536"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTENC, MIM, and some RF Regularization (1)\n",
    "Everything from the last run, but:\n",
    "* Ensemble Regularization: trees increased: 100 -> 10k\n",
    "* Depth Regularization:  max_depth: none -> 3\n",
    "\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 10k trees\n",
    "* Missing Data\n",
    "    - Missing Indictor Method (for every colX w/ missing data, another column is\n",
    "      created, colX_missing, which flags all missing data in colX as 1 and non-missing\n",
    "      data as 0; the missing data in colX are then zeroed out)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using SMOTENC\n",
    "* Regularization\n",
    "    - Ensemble Regularization: trees increased: 100 -> 10k\n",
    "    - Depth Regularization:  max_depth: none -> 3\n",
    "* RESULTS\n",
    "    - AUC IMPROVED:  0.53 -> 0.57\n",
    "    - **UPDATE (2020-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have entirely different\n",
    "      results \n",
    "      * impact here: regularization helps take the AUC back to 0.57 (where it began for seed47 on most naive\n",
    "        set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.8656597774244833\n",
      "Val Accuracy: 0.8453237410071942\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.8656597774244833\n",
      "Val Bal Accuracy: 0.5784131834552003\n",
      "-------------------\n",
      "Trn AUROC: 0.8656597774244833\n",
      "Val AUROC: 0.5784131834552004\n",
      "-------------------\n",
      "Trn AUPRC: 0.8085425177221537\n",
      "Val AUPRC: 0.04019777476603974\n",
      "-------------------\n",
      "Trn Precision Score: 0.8671445357209021\n",
      "Val Precision Score: 0.9469715844234095\n",
      "-------------------\n",
      "Trn Recall Score: 0.8656597774244833\n",
      "Val Recall Score: 0.8453237410071942\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1049  209]\n",
      " [ 129 1129]]\n",
      "Val Confusion:\n",
      " [[465  74]\n",
      " [ 12   5]]\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.102981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.066682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>admh_hxft2</td>\n",
       "      <td>0.048813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>0.042232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>0.040882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0.039613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>0.019226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admh_hxmc2</td>\n",
       "      <td>0.018280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>0.016564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>advs_prepregbmi</td>\n",
       "      <td>0.016064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           x_col  gini_importance\n",
       "1          adscmat_educ_combohs4         0.102981\n",
       "2                   adsc_gender2         0.066682\n",
       "3                     admh_hxft2         0.048813\n",
       "4           adscmat_toiletwater2         0.042232\n",
       "5   alcsmk_t1_avecigs.wk_missing         0.040882\n",
       "6   alcsmk_t2_avecigs.wk_missing         0.039613\n",
       "7   adscmat_grossincome7_missing         0.019226\n",
       "8                     admh_hxmc2         0.018280\n",
       "9                   admh_anemia3         0.016564\n",
       "10               advs_prepregbmi         0.016064"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=10000, random_state=seed,\n",
    "                            n_jobs=-1, max_depth=3)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTENC, MIM, and some RF Regularization (2)\n",
    "Tightened Depth Regularization:  3 -> 2\n",
    "\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 10k trees\n",
    "* Missing Data\n",
    "    - Missing Indictor Method (for every colX w/ missing data, another column is\n",
    "      created, colX_missing, which flags all missing data in colX as 1 and non-missing\n",
    "      data as 0; the missing data in colX are then zeroed out)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using SMOTENC\n",
    "* Regularization\n",
    "    - Ensemble Regularization: trees increased: 100 -> 10k\n",
    "    - Depth Regularization:  max_depth: 3 -> 2\n",
    "* RESULTS\n",
    "    - **AUC IMPROVED:  0.57 -> 0.60**\n",
    "    - **UPDATE (2020-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have entirely different\n",
    "      results \n",
    "      * Impact Here:  Instead of AUC improving, it now shot down to 0.5 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.794912559618442\n",
      "Val Accuracy: 0.7122302158273381\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.7949125596184419\n",
      "Val Bal Accuracy: 0.5097675433809887\n",
      "-------------------\n",
      "Trn AUROC: 0.7949125596184421\n",
      "Val AUROC: 0.5097675433809887\n",
      "-------------------\n",
      "Trn AUPRC: 0.7192492979315582\n",
      "Val AUPRC: 0.031194421625329493\n",
      "-------------------\n",
      "Trn Precision Score: 0.8087151278469225\n",
      "Val Precision Score: 0.9415574240359574\n",
      "-------------------\n",
      "Trn Recall Score: 0.794912559618442\n",
      "Val Recall Score: 0.7122302158273381\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[ 867  391]\n",
      " [ 125 1133]]\n",
      "Val Confusion:\n",
      " [[391 148]\n",
      " [ 12   5]]\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.091838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.061818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>0.045576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0.043981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>0.039206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>admh_hxft2</td>\n",
       "      <td>0.030137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>0.020792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admh_hx_baby_neuraltube2_missing</td>\n",
       "      <td>0.017793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>admh_hxab2_missing</td>\n",
       "      <td>0.017403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>admh_hxsidssuid2_missing</td>\n",
       "      <td>0.016997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               x_col  gini_importance\n",
       "1              adscmat_educ_combohs4         0.091838\n",
       "2                       adsc_gender2         0.061818\n",
       "3       alcsmk_t1_avecigs.wk_missing         0.045576\n",
       "4       alcsmk_t2_avecigs.wk_missing         0.043981\n",
       "5               adscmat_toiletwater2         0.039206\n",
       "6                         admh_hxft2         0.030137\n",
       "7       adscmat_grossincome7_missing         0.020792\n",
       "8   admh_hx_baby_neuraltube2_missing         0.017793\n",
       "9                 admh_hxab2_missing         0.017403\n",
       "10          admh_hxsidssuid2_missing         0.016997"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=10000, n_jobs=-1, random_state=seed,\n",
    "                            max_depth=2)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTENC, MIM, and some RF Regularization (3)\n",
    "Tightened Depth Regularization:  2 -> 1\n",
    "Increased Ensemble Regularziation:\n",
    "* 1st Run: no increase\n",
    "* 2nd Run: 10k trees -> 50k trees\n",
    "\n",
    "\n",
    "Set Up:\n",
    "* Model: RandomForestClassifier\n",
    "    - 100 trees\n",
    "* Missing Data\n",
    "    - Missing Indictor Method (for every colX w/ missing data, another column is\n",
    "      created, colX_missing, which flags all missing data in colX as 1 and non-missing\n",
    "      data as 0; the missing data in colX are then zeroed out)\n",
    "* Target: adsc_dlvrybefore34wks\n",
    "* Data: toif1pwr_demotoif1pwr_df\n",
    "    - democlinical + Toitu Power Cols\n",
    "* Col Drop: \n",
    "    - remove sensor_toi_f1_mvt_pwr_mvt41 \n",
    "* SMOTE\n",
    "    - using SMOTENC\n",
    "* Regularization\n",
    "    - Ensemble Regularization: trees increased: \n",
    "        - 1st Run:  No change (10k trees)\n",
    "        - 2nd Run:  10k trees -> 50k trees\n",
    "    - Depth Regularization:  max_depth: 2 -> 1\n",
    "* RESULTS\n",
    "    - **TOO MUCH REGULARIZATION:  AUC: 0.6 -> 0.58 in both runs**\n",
    "    - **UPDATE (2020-Jan-27)**: found out that, yet again, these results depend on random seed; for example,\n",
    "      in my last run, I updated all random seeds to 47 -- and suddenly we have entirely different\n",
    "      results \n",
    "      * Impact Here:  AUC remaining around 0.5 AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.6665341812400636\n",
      "Val Accuracy: 0.48381294964028776\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.6665341812400636\n",
      "Val Bal Accuracy: 0.5058932663974681\n",
      "-------------------\n",
      "Trn AUROC: 0.6665341812400636\n",
      "Val AUROC: 0.505893266397468\n",
      "-------------------\n",
      "Trn AUPRC: 0.6020954289739409\n",
      "Val AUPRC: 0.030932606855691918\n",
      "-------------------\n",
      "Trn Precision Score: 0.7145239182476096\n",
      "Val Precision Score: 0.9414419024481908\n",
      "-------------------\n",
      "Trn Recall Score: 0.6665341812400636\n",
      "Val Recall Score: 0.48381294964028776\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[ 541  717]\n",
      " [ 122 1136]]\n",
      "Val Confusion:\n",
      " [[260 279]\n",
      " [  8   9]]\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_col</th>\n",
       "      <th>gini_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>0.06954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>0.06272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>0.04974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0.04882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>0.03546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>admh_hx_baby_neuraltube2_missing</td>\n",
       "      <td>0.02466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>admh_gdmprior2_missing</td>\n",
       "      <td>0.02380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>admh_hx_baby_iugrsga2_missing</td>\n",
       "      <td>0.02328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>admh_hxsids2_missing</td>\n",
       "      <td>0.02312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>admh_hx_baby_cleft2_missing</td>\n",
       "      <td>0.02308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               x_col  gini_importance\n",
       "1              adscmat_educ_combohs4          0.06954\n",
       "2                       adsc_gender2          0.06272\n",
       "3       alcsmk_t1_avecigs.wk_missing          0.04974\n",
       "4       alcsmk_t2_avecigs.wk_missing          0.04882\n",
       "5               adscmat_toiletwater2          0.03546\n",
       "6   admh_hx_baby_neuraltube2_missing          0.02466\n",
       "7             admh_gdmprior2_missing          0.02380\n",
       "8      admh_hx_baby_iugrsga2_missing          0.02328\n",
       "9               admh_hxsids2_missing          0.02312\n",
       "10       admh_hx_baby_cleft2_missing          0.02308"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "rf = RandomForestClassifier(n_estimators=50000, n_jobs=-1, max_depth=1, random_state=seed)\n",
    "rf.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = rf.predict(x_trn)\n",
    "yp_val = rf.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "imp = zip(x.columns, rf.feature_importances_)\n",
    "pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "    sort_values(by='gini_importance',ascending=False).\\\n",
    "    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, ElasticNet, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIM w/ LASSO\n",
    "Here I do a LASSO Regression with rounding.  It's not a recommended technique, but \n",
    "something interesting happened:  despite having more false negatives, we have a lot\n",
    "more true positives.  This motivates a logistic regression w/ L1 Penalty next.\n",
    "\n",
    "\n",
    "NOTE: the random seed effect is also present here.  For example, when I initially\n",
    "developed these notebooks, I used random seed 23 in `train_test_split` and 37 in `SMOTENC`. This\n",
    "resulted in AUC 0.556.  HOWEVER, when I changed both seeds to 47 (like in above RF runs), the\n",
    "AUC bumped up to 0.597.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.7130365659777425\n",
      "Val Accuracy: 0.6546762589928058\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.7130365659777425\n",
      "Val Bal Accuracy: 0.5940194259521991\n",
      "-------------------\n",
      "Trn AUROC: 0.7135142717766866\n",
      "Val AUROC: 0.5967477900251009\n",
      "-------------------\n",
      "Trn AUPRC: 0.6457150274211438\n",
      "Val AUPRC: 0.03938653730908509\n",
      "-------------------\n",
      "Trn Precision Score: 0.7196169363765884\n",
      "Val Precision Score: 0.9495004662619262\n",
      "-------------------\n",
      "Trn Recall Score: 0.7130365659777425\n",
      "Val Recall Score: 0.6546762589928058\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[   0    0    0]\n",
      " [   1  790  467]\n",
      " [   0  254 1004]]\n",
      "Val Confusion:\n",
      " [[  0   0   0]\n",
      " [  2 355 182]\n",
      " [  0   8   9]]\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore34wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = np.round(lasso.predict(x_trn)).astype(int)\n",
    "yp_val = np.round(lasso.predict(x_val)).astype(int)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(yp_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Like I said, this technique isn't really a great one.  As you can see, on training, some\n",
    "targets were rounded down to -1, which we know is impossible.  However, it was still refreshing\n",
    "to see that there was some signal detected here (AUC 0.56 on seed 23(37) run;  AUC 0.60 on seed47 run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIM, SMOTE, L1-Penalized LogReg\n",
    "Holy shit. Wtf. This one is amazing.\n",
    "\n",
    "\n",
    "### UPDATE (2020-Jan-27)\n",
    "What's crazier is that this is more robust against random seed.  This really indicates to\n",
    "me that we were experiencing an extrapolation nightmare scenario with the RF.  Basically, if\n",
    "your training set is a representative sample, then RFs will be amazing; however if any of the\n",
    "important independent vars or the target var is not representative in training, the RF will\n",
    "not work on validation -- which is what we saw.\n",
    "\n",
    "\n",
    "I originally ran this model w/ seed 23 in `train_test_split` and 37 in `SMOTENC`. This\n",
    "resulted AUC of 0.998 on both training and validation; AUPRC of 0.998 on training, but 0.6\n",
    "on validation; and a confusion matrix on validation like:\n",
    "```\n",
    "[[551   2]\n",
    " [  0   3]]\n",
    "```\n",
    "\n",
    "It was so good, I was afraid to change the random seed.  But when I change both to 47, we \n",
    "get very similar results:  trn/val AUC of 0.9996/0.9955;  tran/val AUPRC of 0.999/0.375;  and\n",
    "a confusion matrix on validation like:\n",
    "```\n",
    " [[548   5]\n",
    " [  0   3]]\n",
    "```\n",
    "\n",
    "The results have suffered a little, but only ever-so-slightly.  \n",
    "\n",
    "When I change the seed to 57, we basically recover that slight decrease:\n",
    "```\n",
    "Trn AUROC: 0.9988363072148952\n",
    "Val AUROC: 0.9981916817359856\n",
    "-------------------\n",
    "Trn AUPRC: 0.9976780185758514\n",
    "Val AUPRC: 0.6\n",
    "```\n",
    "\n",
    "Point is, this is the real deal.  The coefficients of the linear model are learning\n",
    "the important relationships in a way that can extrapolate -- something the RF can't do.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9988363072148952\n",
      "Val Accuracy: 0.9964028776978417\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9988363072148952\n",
      "Val Bal Accuracy: 0.9981916817359855\n",
      "-------------------\n",
      "Trn AUROC: 0.9988363072148952\n",
      "Val AUROC: 0.9981916817359856\n",
      "-------------------\n",
      "Trn AUPRC: 0.9976780185758514\n",
      "Val AUPRC: 0.6\n",
      "-------------------\n",
      "Trn Precision Score: 0.9988390092879257\n",
      "Val Precision Score: 0.997841726618705\n",
      "-------------------\n",
      "Trn Recall Score: 0.9988363072148952\n",
      "Val Recall Score: 0.9964028776978417\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1286    3]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[551   2]\n",
      " [  0   3]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=57\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = abs(model.coef_[0])\n",
    "feature_importance = feature_importance / feature_importance.max()\n",
    "rank = np.argsort(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = pd.DataFrame({'ftr': x.columns, 'imp': np.flip(feature_importance[rank]), 'sign': np.sign(np.flip(model.coef_[0][rank]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ftr</th>\n",
       "      <th>imp</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adafppappa_afpmom</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adcdrisc_cdrisc_raw</td>\n",
       "      <td>0.864132</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>addrg_mjlmp2</td>\n",
       "      <td>0.698368</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>addrg_mjt12</td>\n",
       "      <td>0.439981</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addrg_mjt22</td>\n",
       "      <td>0.344187</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>addrg_meth1yrprior6</td>\n",
       "      <td>0.341854</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>addrg_methlmp2</td>\n",
       "      <td>0.293688</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>addrg_metht12</td>\n",
       "      <td>0.252656</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>addrg_metht22</td>\n",
       "      <td>0.160341</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>addrg_other1yrprior2</td>\n",
       "      <td>0.149743</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>addrg_otherlmp2</td>\n",
       "      <td>0.135101</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>addrg_othert12</td>\n",
       "      <td>0.111913</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>addrg_othert22</td>\n",
       "      <td>0.087499</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adedin_edinburgh_raw_all-10-15-20_db</td>\n",
       "      <td>0.085252</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adedin_cycleid_all-10-15-20_db</td>\n",
       "      <td>0.072948</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adelig_mat_age</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adelig_racenih7</td>\n",
       "      <td>0.047193</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adelig_raceai4</td>\n",
       "      <td>0.035372</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adfetalgrowth_deviationindex</td>\n",
       "      <td>0.024841</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>admh_hxft2</td>\n",
       "      <td>0.022562</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>admh_hxnt2</td>\n",
       "      <td>0.021548</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>admh_hxpt2</td>\n",
       "      <td>0.016845</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>admh_hxlb2</td>\n",
       "      <td>0.015726</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>admh_hxsb2</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>admh_hxmc2</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>admh_hxab2</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>admh_hxid2</td>\n",
       "      <td>0.005389</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>admh_hxsuid2</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>admh_hxsids2</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>admh_hxsidssuid2</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>admh_raprior2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>admh_thromboembolicdiseaseprior2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>adptsd_htq_total_raw_cid15-20_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>adptsd_lec_exp_cid15-20_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>adptsd_lec_events_cid15-20_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>adscmat_educ_combohs4_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>adscmat_empl_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>adscmat_empl_comb4_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>adscmat_fertility2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>adsmk_avgnumcighome_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>adstai_s_anxiety_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>adstai_t_anxiety_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>advs_height_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>advs_prepregweight_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>advs_prepregbmi_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>alcsmk_t1_alc_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>alcsmk_t2_alc_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>alcsmk_bt1_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>alcsmk_bt2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>mental_ssri2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>mental_antidepress2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>mental_cantipsych2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>mental_aantipsych2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>mental_moodstab2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>mental_stim2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>mental_antianx2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>mental_anticonv2_missing</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          ftr       imp  sign\n",
       "0                           adafppappa_afpmom  1.000000   1.0\n",
       "1                         adcdrisc_cdrisc_raw  0.864132   1.0\n",
       "2                                addrg_mjlmp2  0.698368  -1.0\n",
       "3                                 addrg_mjt12  0.439981  -1.0\n",
       "4                                 addrg_mjt22  0.344187  -1.0\n",
       "5                         addrg_meth1yrprior6  0.341854  -1.0\n",
       "6                              addrg_methlmp2  0.293688   1.0\n",
       "7                               addrg_metht12  0.252656  -1.0\n",
       "8                               addrg_metht22  0.160341  -1.0\n",
       "9                        addrg_other1yrprior2  0.149743  -1.0\n",
       "10                            addrg_otherlmp2  0.135101   1.0\n",
       "11                             addrg_othert12  0.111913  -1.0\n",
       "12                             addrg_othert22  0.087499  -1.0\n",
       "13       adedin_edinburgh_raw_all-10-15-20_db  0.085252  -1.0\n",
       "14             adedin_cycleid_all-10-15-20_db  0.072948  -1.0\n",
       "15                             adelig_mat_age  0.059443  -1.0\n",
       "16                            adelig_racenih7  0.047193  -1.0\n",
       "17                             adelig_raceai4  0.035372  -1.0\n",
       "18               adfetalgrowth_deviationindex  0.024841  -1.0\n",
       "19                                 admh_hxft2  0.022562  -1.0\n",
       "20                                 admh_hxnt2  0.021548  -1.0\n",
       "21                                 admh_hxpt2  0.016845   1.0\n",
       "22                                 admh_hxlb2  0.015726  -1.0\n",
       "23                                 admh_hxsb2  0.007513  -1.0\n",
       "24                                 admh_hxmc2  0.007001   1.0\n",
       "25                                 admh_hxab2  0.005840  -1.0\n",
       "26                                 admh_hxid2  0.005389  -1.0\n",
       "27                               admh_hxsuid2  0.001751  -1.0\n",
       "28                               admh_hxsids2  0.001332   1.0\n",
       "29                           admh_hxsidssuid2  0.000036   1.0\n",
       "..                                        ...       ...   ...\n",
       "157                     admh_raprior2_missing  0.000000   0.0\n",
       "158  admh_thromboembolicdiseaseprior2_missing  0.000000   0.0\n",
       "159     adptsd_htq_total_raw_cid15-20_missing  0.000000   0.0\n",
       "160           adptsd_lec_exp_cid15-20_missing  0.000000   0.0\n",
       "161        adptsd_lec_events_cid15-20_missing  0.000000   0.0\n",
       "162             adscmat_educ_combohs4_missing  0.000000   0.0\n",
       "163                      adscmat_empl_missing  0.000000   0.0\n",
       "164                adscmat_empl_comb4_missing  0.000000   0.0\n",
       "165                adscmat_fertility2_missing  0.000000   0.0\n",
       "166              adscmat_grossincome7_missing  0.000000   0.0\n",
       "167               adsmk_avgnumcighome_missing  0.000000   0.0\n",
       "168                  adstai_s_anxiety_missing  0.000000   0.0\n",
       "169                  adstai_t_anxiety_missing  0.000000   0.0\n",
       "170                       advs_height_missing  0.000000   0.0\n",
       "171                advs_prepregweight_missing  0.000000   0.0\n",
       "172                   advs_prepregbmi_missing  0.000000   0.0\n",
       "173                     alcsmk_t1_alc_missing  0.000000   0.0\n",
       "174                     alcsmk_t2_alc_missing  0.000000   0.0\n",
       "175                        alcsmk_bt1_missing  0.000000   0.0\n",
       "176                        alcsmk_bt2_missing  0.000000   0.0\n",
       "177              alcsmk_t1_avecigs.wk_missing  0.000000   0.0\n",
       "178              alcsmk_t2_avecigs.wk_missing  0.000000   0.0\n",
       "179                      mental_ssri2_missing  0.000000   0.0\n",
       "180               mental_antidepress2_missing  0.000000   0.0\n",
       "181                mental_cantipsych2_missing  0.000000   0.0\n",
       "182                mental_aantipsych2_missing  0.000000   0.0\n",
       "183                  mental_moodstab2_missing  0.000000   0.0\n",
       "184                      mental_stim2_missing  0.000000   0.0\n",
       "185                   mental_antianx2_missing  0.000000   0.0\n",
       "186                  mental_anticonv2_missing  0.000000   0.0\n",
       "\n",
       "[187 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(106, 'sensor_toi_f1_mvt_pwr_mvt21'), (107, 'sensor_toi_f1_mvt_pwr_mvt31')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, imps.iloc[k,:].ftr) for k in imps.index if 'sensor' in imps.iloc[k,:]['ftr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check:  What Causes Such Great Performance?\n",
    "Below we find that:\n",
    "* SMOTE is absolutely necessary (remove it and the AUC sinks to 0.5)\n",
    "* L1 regularization is critically necessary\n",
    "    - use L2 (penalty='l2') and AUC goes from 0.99 down to 0.66\n",
    "    - remove it (penalty='none') \n",
    "    \n",
    "# 1. Remove SMOTE\n",
    "0.999 AUC?!?\n",
    "\n",
    "Let's take out SMOTE and see what happens.\n",
    "\n",
    "**Spoiler**:  AUC goes from 0.999 -> 0.5.  (SMOTE is amazing, eh?)\n",
    "\n",
    "These results are robust:\n",
    "* try un-commenting the SMOTENC lines: the AUC will shoot back up to 0.99\n",
    "* try changing the random seed:  results will remain similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9984579799537394\n",
      "Val Accuracy: 0.987410071942446\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.875\n",
      "Val Bal Accuracy: 0.49638336347197104\n",
      "-------------------\n",
      "Trn AUROC: 0.875\n",
      "Val AUROC: 0.49638336347197104\n",
      "-------------------\n",
      "Trn AUPRC: 0.7515420200462606\n",
      "Val AUPRC: 0.00539568345323741\n",
      "-------------------\n",
      "Trn Precision Score: 0.9984603688306508\n",
      "Val Precision Score: 0.9891988583046605\n",
      "-------------------\n",
      "Trn Recall Score: 0.9984579799537394\n",
      "Val Recall Score: 0.987410071942446\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1289    0]\n",
      " [   2    6]]\n",
      "Val Confusion:\n",
      " [[549   4]\n",
      " [  3   0]]\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "#cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "#sm = SMOTENC(cats, random_state=seed)\n",
    "#x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adsc_dlvrybefore28wks    0.539568\n",
       "dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100* y_val.sum()/y_val.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Swap Out L1 with with other penalties\n",
    "\n",
    "Here we put SMOTE back in, keep the MIM, but swap out L1 with L2-Penalized LogReg.\n",
    "\n",
    "\n",
    "**SPOILER**:  \n",
    "* L2 sucks here.  It's all about the L1 Penalty.\n",
    "* To be more specific:  AUC on validation goes down to 0.66 (from 0.99)\n",
    "\n",
    "\n",
    "## 2a: NO PENALTY (no rescaling)\n",
    "**2020-Jan-27**\n",
    "\n",
    "The `liblinear` solver does not work for `penalty=none` (logistic regressions default to\n",
    "L2 penalties in sklearn), so we are forced to explore the other solvers:\n",
    "* newton-cg\n",
    "* lbfgs\n",
    "* sag\n",
    "* saga\n",
    "\n",
    "I ran through each of them, recording the results below.  The `newton-cg` method\n",
    "still did fairly well (AUC on validation of 0.83), though all other solvers did\n",
    "terrible (worse than chance, i.e., smaller than 0.5 AUC).  From my learnings a week\n",
    "or so ago, I already knew the SAGA needed variables to be rescaled to promise\n",
    "convergence; I've not read that this is true for SAG as well.  I'm guessing this is\n",
    "true for LBFGS as well.  \n",
    "\n",
    "So, below I report non-scaled results.  Next, I'll rescale all continuous variables\n",
    "and report again.\n",
    "\n",
    "Newton-CG\n",
    "```\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8306208559373116\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.2684652278177458\n",
    "```\n",
    "\n",
    "LBFGS w/o rescaling\n",
    "```\n",
    "Trn AUROC: 0.965477114041893\n",
    "Val AUROC: 0.47106690777576854\n",
    "-------------------\n",
    "Trn AUPRC: 0.9374938857783774\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```\n",
    "\n",
    "\n",
    "Sag w/ rescaling\n",
    "```\n",
    "Trn AUROC: 0.8855702094647014\n",
    "Val AUROC: 0.4113924050632911\n",
    "-------------------\n",
    "Trn AUPRC: 0.8238578777172731\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```\n",
    "\n",
    "Saga w/o rescaling\n",
    "```\n",
    "Trn AUROC: 0.865399534522886\n",
    "Val AUROC: 0.3987341772151899\n",
    "-------------------\n",
    "Trn AUPRC: 0.7994590486456217\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.9928057553956835\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.8306208559373116\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.8306208559373116\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.2684652278177458\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.994957500424343\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.9928057553956835\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1289    0]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[550   3]\n",
      " [  1   2]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(penalty='none', solver='newton-cg')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(penalty='none', solver='newton-cg')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: NO PENALTY (with rescaled ind vars)\n",
    "**2020-Jan-27**\n",
    "\n",
    "Here, we use some rescale methods on the independent variables in order\n",
    "to get variables on a similar scale...which is a requirement for convergence\n",
    "for the SAG and SAGA solvers (and maybe LBFGS too).  \n",
    "\n",
    "LBFGS improves, but SAG and SAGA remain sucky.  This might be due to using\n",
    "bad stopping conditions... But I'm not sure.  There are related meta parameters\n",
    "that we can play with to find out:\n",
    "* `max_iter` is the number of iterations a solver goes through until it is\n",
    "   assumed that the solver has converged; it defaults to 100\n",
    "* `tol` is the tolerance for stopping criteria; it defaults to `1e-4`\n",
    "\n",
    "We can experiment with these things in the next section.  Here, we leave them defaulted.\n",
    "\n",
    "Newton-CG w/ rescale (default stopping conditions)\n",
    "```\n",
    "NCG w/ Standardizer ( (z-avg)/std )\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8297\n",
    "-------------------\n",
    "Val AUROC: 1.0\n",
    "Val AUPRC: 0.2240\n",
    "\n",
    "NCG w/ Normalizer ( (z-min)/(max-min) )\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8288\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.1923\n",
    "```\n",
    "\n",
    "LBFGS w/ rescale (default stopping conditions)\n",
    "* this one is worse on validation, and actually just\n",
    "  ever so slightly worse on training (it's not perfect)\n",
    "* these shortcomings might have to do with the solver itself,\n",
    "  or some meta parameters (stopping conditions)\n",
    "\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.9976726144297904\n",
    "Val AUROC: 0.6621458710066305\n",
    "-------------------\n",
    "Trn AUPRC: 0.9965143383682782\n",
    "Val AUPRC: 0.059152677857713824\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.9584949573312646\n",
    "Val AUROC: 0.4629294755877034\n",
    "-------------------\n",
    "Trn AUPRC: 0.923352435530086\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```\n",
    "\n",
    "SAG w/ rescale (default stopping conditions)\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.6586501163692785\n",
    "Val AUROC: 0.3291139240506329\n",
    "-------------------\n",
    "Trn AUPRC: 0.6036276415535662\n",
    "Val AUPRC: 0.00539568345323741\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.9193173002327386\n",
    "Val AUROC: 0.4358047016274864\n",
    "-------------------\n",
    "Trn AUPRC: 0.8643624049121339\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```\n",
    "\n",
    "SAGA w/ rescale (default stopping conditions)\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.6551590380139644\n",
    "Val AUROC: 0.3381555153707052\n",
    "-------------------\n",
    "Trn AUPRC: 0.6020140131824097\n",
    "Val AUPRC: 0.00539568345323741\n",
    "\n",
    "w/ Normazlizer\n",
    "Trn AUROC: 0.9053529868114819\n",
    "Val AUROC: 0.42495479204339964\n",
    "-------------------\n",
    "Trn AUPRC: 0.8482413976043657\n",
    "Val AUPRC: 0.00539568345323741\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standardizer: (z - avg_trn) / std_trn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.9910071942446043\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.8297166968053044\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.8297166968053044\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.22402078337330134\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.9945945062132112\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.9910071942446043\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1289    0]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[549   4]\n",
      " [  1   2]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "scaler='s'  # standardizer or normalizer\n",
    "solver='newton-cg'\n",
    "penalty='none'\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "# Categoricals & Non-Categoricals\n",
    "# -- NOTE: this \"10\" split works on this data set\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= 10]\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "## NOTE (2020-Jan-27): this \"0-1\" scaler actually hurts \n",
    "##   the AUC, while the standardizer does not...(?)\n",
    "##### NOTE2SELF:  when using SAG or SAGA solvers, you have to scale.\n",
    "#for col in nots:\n",
    "#    min_trn = x_trn[col].min()\n",
    "#    max_trn = x_trn[col].max()\n",
    "#    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "#    x_trn[col] = x_trn[col].map(scale)\n",
    "#    x_val[col] = x_val[col].map(scale)\n",
    "if scaler[0].lower()=='s':\n",
    "    print('Using Standardizer: (z - avg_trn) / std_trn')\n",
    "    scale = lambda z: (z - avg_trn) / std_trn\n",
    "elif scaler[0].lower()=='n':\n",
    "    print('Using Normalizer: (z - min_trn) / (max_trn - min_trn)')\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "for col in nots:\n",
    "    avg_trn = x_trn[col].mean()\n",
    "    std_trn = x_trn[col].std()\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(penalty=penalty, solver=solver)\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b: NO PENALTY (with rescaled ind vars, revised stopping conditions)\n",
    "**2020-Jan-27**\n",
    "\n",
    "Here, explore we why SAG and SAGA remain sucky, assuming it might be due to\n",
    "poorly planned stopping conditions.\n",
    "\n",
    "Newton-CG works good for everything, so I focus more on LBFGS, SAG, and SAGA.\n",
    "\n",
    "We have two major hyper parameters that control this:\n",
    "* `max_iter` is the number of iterations a solver goes through until it is\n",
    "   assumed that the solver has converged; it defaults to 100\n",
    "* `tol` is the tolerance for stopping criteria; it defaults to `1e-4`\n",
    "\n",
    "\n",
    "MAJOR RESULTS\n",
    "* <font color='red'>though LBFGS remained fairly suboptimal around 0.66 AUC, I was able to\n",
    "  get the SAG and SAGA solvers to perform at ~0.83 AUC, which is what Newton-CG\n",
    "  was able to get without rescaling and using default hyperparameters</font>\n",
    "* <font color='red'>the \"normalizer\" has dramatically worse performance than the \"standardizer\"\n",
    "  on SAG and SAGA </font>\n",
    "  - it might be salvageable with more hyperparam tweaking, but ... why bother?\n",
    "  - **ODDLY**, using the Newton-CG solver, the situation is reversed:  the \"normalizer\"\n",
    "    works its wonders, while the \"standardizer\" results in MUCH poorer performance\n",
    "    \n",
    "\n",
    "-------------------------------\n",
    "\n",
    "\n",
    "LBFGS w/ rescale (default stopping conditions)\n",
    "\n",
    "this one is worse on validation, and actually just ever so slightly worse on training (it's not perfect)\n",
    "these shortcomings might have to do with the solver itself, or some meta parameters (stopping conditions)\n",
    "\n",
    "LBFGS\n",
    "* it is very hard to improve LBFGS\n",
    "    - below I show one tweak, but I tried MANY, never getting better (if at all)\n",
    "    \n",
    "```\n",
    "w/ Standardizer (from last time, max_iter=100)\n",
    "Trn AUROC: 0.9976726144297904\n",
    "Val AUROC: 0.6621458710066305\n",
    "-------------------\n",
    "Trn AUPRC: 0.9965143383682782\n",
    "Val AUPRC: 0.059152677857713824\n",
    "\n",
    "w/ Standardizer (this time, max_iter=5000)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.6630500301386376\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.07026378896882494\n",
    "```\n",
    "\n",
    "\n",
    "SAG w/ rescale (default stopping conditions)\n",
    "* was able to get SAG up to ~0.83 AUC, which is what Newton-CG gets\n",
    "* however, the \"normalizer\" hurts performance\n",
    "    - use \"standardizer\"\n",
    "\n",
    "```\n",
    "w/ Standardizer (from last time, max_iter=100, tol=1e-4)\n",
    "Trn AUROC: 0.6586501163692785\n",
    "Val AUROC: 0.3291139240506329\n",
    "-------------------\n",
    "Trn AUPRC: 0.6036276415535662\n",
    "Val AUPRC: 0.00539568345323741\n",
    "\n",
    "*** BETTER ***\n",
    "w/ Standardizer (max_iter=500, tol=1e-4)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.6621458710066305\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.059152677857713824\n",
    "\n",
    "\n",
    "###########################################\n",
    "#########   THIS ONE   ####################\n",
    "###########################################\n",
    "*** BETTER ***\n",
    "w/ Standardizer (max_iter=5000, tol=1e-4)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8288125376732971\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.1922747516272696\n",
    "\n",
    "\n",
    "*** SAME ***\n",
    "w/ Standardizer (max_iter=50k, tol=1e-4)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8288125376732971\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.1922747516272696\n",
    "\n",
    "*** WORSE ***  (better than original, but worse than best run)\n",
    "w/ Standardizer (max_iter=5000, tol=1e-5)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.6621458710066305\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.059152677857713824\n",
    "\n",
    "==========================================\n",
    "\n",
    "w/ Normalizer (max_iter=5000, tol=1e-4)\n",
    "Trn AUROC: 0.9375484871993794\n",
    "Val AUROC: 0.6133212778782399      \n",
    "-------------------\n",
    "Trn AUPRC: 0.8889655172413793\n",
    "Val AUPRC: 0.009152677857713828\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SAGA w/ rescale (default stopping conditions)\n",
    "* using SAG's best hyperparameters, we also can get SAGA up to ~0.83 AUC, which is what Newton-CG gets\n",
    "* again, the \"normalizer\" hurts performance\n",
    "\n",
    "```\n",
    "w/ Standardizer (from last time: max_iter=100, tol=1e-4)\n",
    "Trn AUROC: 0.6551590380139644\n",
    "Val AUROC: 0.3381555153707052\n",
    "-------------------\n",
    "Trn AUPRC: 0.6020140131824097\n",
    "Val AUPRC: 0.00539568345323741\n",
    "\n",
    "\n",
    "w/ Standardizer (max_iter=5000, tol=1e-4)\n",
    "Trn AUROC: 1.0\n",
    "Val AUROC: 0.8288125376732971  #   <------ NICE\n",
    "-------------------\n",
    "Trn AUPRC: 1.0\n",
    "Val AUPRC: 0.1922747516272696\n",
    "\n",
    "==========================================\n",
    "\n",
    "w/ Normalizer (max_iter=5000, tol=1e-4)\n",
    "Trn AUROC: 0.9294026377036462\n",
    "Val AUROC: 0.6088004822182037  #  <------- Ick! Ew!\n",
    "-------------------\n",
    "Trn AUPRC: 0.876274643099932\n",
    "Val AUPRC: 0.008725327430363403\n",
    "```\n",
    "\n",
    "\n",
    "See this post for more info on the Solvers:\n",
    "* https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions\n",
    "\n",
    "LBFGS probably is the worst b/c it is an approximation of Newton-CG...but doesn't seem\n",
    "to have anything else going for it.  The SAG and SAGA methods are stochastic gradient\n",
    "descent methods, so it's not surprising that (i) we had to normalize, and (ii) they ultimately\n",
    "work.\n",
    "    \n",
    "    \n",
    "HERE's something weird:  \n",
    "* though the \"standardizer\" works so well for SAG and SAGA, while the\n",
    "  \"normalizer\" is terrible, the situation is reversed for Newton-CG\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standardizer: (z - avg_trn) / std_trn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 1.0\n",
      "Val Accuracy: 0.987410071942446\n",
      "-------------------\n",
      "Trn Bal Accuracy: 1.0\n",
      "Val Bal Accuracy: 0.6621458710066305\n",
      "-------------------\n",
      "Trn AUROC: 1.0\n",
      "Val AUROC: 0.6621458710066305\n",
      "-------------------\n",
      "Trn AUPRC: 1.0\n",
      "Val AUPRC: 0.059152677857713824\n",
      "-------------------\n",
      "Trn Precision Score: 1.0\n",
      "Val Precision Score: 0.9918868541530412\n",
      "-------------------\n",
      "Trn Recall Score: 1.0\n",
      "Val Recall Score: 0.987410071942446\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1289    0]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[548   5]\n",
      " [  2   1]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "scaler='s'  # standardizer or normalizer\n",
    "solver='newton-cg'  # liblinear, newton-cg, lbfgs, sag, saga\n",
    "penalty='none'\n",
    "tol=1e-4  # default: 1e-4\n",
    "max_iter=5000 # default: 100\n",
    "cat_thresh=3\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "# Categoricals & Non-Categoricals\n",
    "# -- NOTE: this \"10\" split works on this data set, but so does \"3\"; oddly\n",
    "#      certain numbers will crash the AUC on some solvers...\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < cat_thresh]\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= cat_thresh]\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "## NOTE (2020-Jan-27): this \"0-1\" scaler sometimes hurts \n",
    "##   the AUC, while the standardizer does not...(?)\n",
    "##### NOTE2SELF:  when using SAG or SAGA solvers, you have to scale.\n",
    "if scaler[0].lower()=='s':\n",
    "    print('Using Standardizer: (z - avg_trn) / std_trn')\n",
    "    scale = lambda z: (z - avg_trn) / std_trn\n",
    "elif scaler[0].lower()=='n':\n",
    "    print('Using Normalizer: (z - min_trn) / (max_trn - min_trn)')\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "for col in nots:\n",
    "    avg_trn = x_trn[col].mean()\n",
    "    std_trn = x_trn[col].std()\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(\n",
    "    penalty=penalty, \n",
    "    solver=solver,\n",
    "    tol = tol,\n",
    "    max_iter = max_iter,\n",
    ")\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c: L2 PENALTY\n",
    "\n",
    "\n",
    "For `penalty=none`, the `standardizer` was the go-to scaler for `sag` and\n",
    "`saga`, while the `normalizer` seemed to be the go-to scaler for `newton-cg`.\n",
    "\n",
    "For `penalty=l2`, everything seems to prefer the `standardizer`.\n",
    "\n",
    "So many of the solvers got the SAME EXACT results that I thought something might be wrong, but\n",
    "then SAG and SAGA gave slightly different (worse) values when using the \"normalizer.\"  I then\n",
    "also did a sanity check and changed `penalty=l2` to `penalty=l1` for the SAGA solver:\n",
    "* for \"normalizer\", I got crappy disheartening results (AUC ~0.60)\n",
    "* for \"standardizer\", I recovered the GREAT results that we got using L1 way above (AUC ~0.99)\n",
    "\n",
    "\n",
    "\n",
    "Newton-CG\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.6630500301386376\n",
    "```\n",
    "\n",
    "Liblinear\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.6630500301386376\n",
    "```\n",
    "\n",
    "\n",
    "LBFGS\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.6630500301386376\n",
    "```\n",
    "\n",
    "SAG\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.9375484871993794\n",
    "Val AUROC: 0.6133212778782399\n",
    "```\n",
    "\n",
    "SAGA\n",
    "```\n",
    "w/ Standardizer\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "\n",
    "w/ Normalizer\n",
    "Trn AUROC: 0.9294026377036462\n",
    "Val AUROC: 0.6088004822182037\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standardizer: (z - avg_trn) / std_trn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9996121024049651\n",
      "Val Accuracy: 0.9892086330935251\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9996121024049651\n",
      "Val Bal Accuracy: 0.8288125376732971\n",
      "-------------------\n",
      "Trn AUROC: 0.999612102404965\n",
      "Val AUROC: 0.8288125376732971\n",
      "-------------------\n",
      "Trn AUPRC: 0.9992248062015504\n",
      "Val AUPRC: 0.1922747516272696\n",
      "-------------------\n",
      "Trn Precision Score: 0.9996124031007751\n",
      "Val Precision Score: 0.9943342749687837\n",
      "-------------------\n",
      "Trn Recall Score: 0.9996121024049651\n",
      "Val Recall Score: 0.9892086330935251\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1288    1]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[548   5]\n",
      " [  1   2]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "scaler='s'  # standardizer or normalizer\n",
    "solver='saga'  # liblinear, newton-cg, lbfgs, sag, saga\n",
    "penalty='l2'\n",
    "tol=1e-4  # default: 1e-4\n",
    "max_iter=5000 # default: 100\n",
    "cat_thresh=3\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "# Categoricals & Non-Categoricals\n",
    "# -- NOTE: this \"10\" split works on this data set, but so does \"3\"; oddly\n",
    "#      certain numbers will crash the AUC on some solvers...\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < cat_thresh]\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= cat_thresh]\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "## NOTE (2020-Jan-27): this \"0-1\" scaler sometimes hurts \n",
    "##   the AUC, while the standardizer does not...(?)\n",
    "##### NOTE2SELF:  when using SAG or SAGA solvers, you have to scale.\n",
    "if scaler[0].lower()=='s':\n",
    "    print('Using Standardizer: (z - avg_trn) / std_trn')\n",
    "    scale = lambda z: (z - avg_trn) / std_trn\n",
    "elif scaler[0].lower()=='n':\n",
    "    print('Using Normalizer: (z - min_trn) / (max_trn - min_trn)')\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "for col in nots:\n",
    "    avg_trn = x_trn[col].mean()\n",
    "    std_trn = x_trn[col].std()\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(\n",
    "    penalty=penalty, \n",
    "    solver=solver,\n",
    "    tol = tol,\n",
    "    max_iter = max_iter,\n",
    ")\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D: ElasticNet\n",
    "\n",
    "Ok, this one is weird... Results aren't changing as I toggle the parameter\n",
    "between L1 and L2 penalties.\n",
    "\n",
    "**Update**:  found out for \"saga\" solver, features must all be of similar scale, so I've \n",
    "been trying to normalize things...but still not working....\n",
    "\n",
    "### UPDATE (2020-Jan-27)\n",
    "The `penalty='elasticnet'` setting only works with the SAGA solver.\n",
    "\n",
    "Here, we must introduce the ElasticNet mixing parameter:\n",
    "* l1_ratio (float, default=None):  the Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if   \n",
    "  `penalty='elasticnet'`. Setting `l1_ratio=0` is equivalent to using `penalty='l2'`, while setting \n",
    "  `l1_ratio=1` is equivalent to using `penalty='l1'`. For 0 < l1_ratio <1, the penalty is a combination \n",
    "  of L1 and L2.\n",
    "\n",
    "\n",
    "First thing to check is if it converges on our L1 and L2 results:\n",
    "```\n",
    "L1 (l1_ratio=1)\n",
    "Trn AUROC: 0.9988363072148954\n",
    "Val AUROC: 0.9954792043399638\n",
    "\n",
    "L2 (l1_ratio=0)\n",
    "Trn AUROC: 0.999612102404965\n",
    "Val AUROC: 0.8288125376732971\n",
    "```\n",
    "\n",
    "YES!  Results are reproduced.\n",
    "\n",
    "Now for more interesting l1 ratios...\n",
    "\n",
    "\n",
    "Random Seed: 47\n",
    "\n",
    "| L1 Ratio | Trn AUROC | Val AUROC | Trn AUPRC | Val AUPRC |\n",
    "|-----------|---------|--------|------------|--------------|\n",
    "| 1.0 | 0.999 | 0.995 | 1.000 | 0.829 |\n",
    "| 0.9 | 0.999 | 0.995 | 0.998 | 0.375 |\n",
    "| 0.8 | 0.999 | 0.829 | 0.998 | 0.192 |\n",
    "| 0.7 | 0.999 | 0.829 | 0.998 | 0.192 |\n",
    "| 0.6 | 0.999 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.5 | 0.999 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.4 | 1.000 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.3 | 1.000 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.2 | 1.000 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.1 | 1.000 | 0.829 | 0.999 | 0.192 |\n",
    "| 0.0 | 1.000 | 0.829 | 0.999 | 0.192 |\n",
    "\n",
    "<font color='red'>Basically, what we find is rapid degradation in performance one we abandon\n",
    "the L1 penalization.  By the time the L1 Ratio gets to 0.8, the performance\n",
    "matches that of L2 penalization (`l1_ratio=0`).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standardizer: (z - avg_trn) / std_trn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9278510473235065\n",
      "Val Accuracy: 0.8812949640287769\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9278510473235067\n",
      "Val Bal Accuracy: 0.6088004822182037\n",
      "-------------------\n",
      "Trn AUROC: 0.9278510473235067\n",
      "Val AUROC: 0.6088004822182037\n",
      "-------------------\n",
      "Trn AUPRC: 0.8738983050847458\n",
      "Val AUPRC: 0.008725327430363403\n",
      "-------------------\n",
      "Trn Precision Score: 0.9369491525423729\n",
      "Val Precision Score: 0.9906359856498321\n",
      "-------------------\n",
      "Trn Recall Score: 0.9278510473235065\n",
      "Val Recall Score: 0.8812949640287769\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1103  186]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[489  64]\n",
      " [  2   1]]\n",
      "-------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=47\n",
    "scaler='s'  # standardizer or normalizer\n",
    "solver='saga'  # liblinear, newton-cg, lbfgs, sag, saga\n",
    "penalty='elasticnet'\n",
    "l1_ratio = 0.1\n",
    "tol=1e-4  # default: 1e-4\n",
    "max_iter=5000 # default: 100\n",
    "cat_thresh=3\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "# Categoricals & Non-Categoricals\n",
    "# -- NOTE: this \"10\" split works on this data set, but so does \"3\"; oddly\n",
    "#      certain numbers will crash the AUC on some solvers...\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < cat_thresh]\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= cat_thresh]\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "## NOTE (2020-Jan-27): this \"0-1\" scaler sometimes hurts \n",
    "##   the AUC, while the standardizer does not...(?)\n",
    "##### NOTE2SELF:  when using SAG or SAGA solvers, you have to scale.\n",
    "if scaler[0].lower()=='s':\n",
    "    print('Using Standardizer: (z - avg_trn) / std_trn')\n",
    "    avg_trn = x_trn[col].mean()\n",
    "    std_trn = x_trn[col].std()\n",
    "    scale = lambda z: (z - avg_trn) / std_trn\n",
    "elif scaler[0].lower()=='n':\n",
    "    print('Using Normalizer: (z - min_trn) / (max_trn - min_trn)')\n",
    "    min_trn = x_trn[col].min()\n",
    "    max_trn = x_trn[col].max()\n",
    "    scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "for col in nots:\n",
    "    x_trn[col] = x_trn[col].map(scale)\n",
    "    x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(\n",
    "    penalty=penalty, \n",
    "    solver=solver,\n",
    "    tol = tol,\n",
    "    max_iter = max_iter,\n",
    "    l1_ratio = l1_ratio,\n",
    ")\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the MIM\n",
    "**2020-Jan-28**\n",
    "\n",
    "Ok, so we know:\n",
    "* we must use SMOTENC\n",
    "* we should absolutely choose pure L1 penalty\n",
    "* seems fairly robust against random seed (but we should explore this more below)\n",
    "* MIM seems to be working just fine...but is it necessary?\n",
    "\n",
    "Here, we will use the best model, but test taking out MIM.\n",
    "\n",
    "**UPDATE**:  this section was supposed to be about removing the MIM, however I \n",
    "quickly found out that there was an odd random seed dependency that remained\n",
    "(though not as severe as before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Reference:  Pipeline w/ MIM\n",
    "Here, we just produce the good results with everything in harmony (SMOTENC, MIM,\n",
    "L1-Penalized Logistic Regression).  \n",
    "\n",
    "The only thing that changed is where I apply the MIM.  Above, it was being\n",
    "applied before rescaling, which means though we say \"missing values are recoded\n",
    "as 0,\" they effectively are recoded as `-mean(colX)/std(colX)`.  From what I read\n",
    "about MIM, this is actually fine (any constant can be used -- missing column's \n",
    "coefficient will just readjust), but conceptually it's a little uglier (there is\n",
    "something more aesthetic about zeroing out `colX` when `colX_missing` kicks in).  \n",
    "So, in order to retain the \"recode to 0\" aesthetic and to have the code match\n",
    "the Eglish I use to explain what I'm doing, I've moved the application of MIM to\n",
    "after variables are rescaled.  (This means I have to apply MIM to subsets instead\n",
    "of the full set at once.)\n",
    "\n",
    "For some reason, this reduced performance:  `0.995 AUC --> 0.83`.  However, the\n",
    "performance was able to be picked back up by changing the tolerance:  `1e-4 --> 1e-3`. \n",
    "(This is assuming we keep solver as liblinear.)\n",
    "\n",
    "\n",
    "Actually, after a few model runs and lots of \"debugging\", I realized that this\n",
    "model tends to oscillate between 0.999 AUC and 0.83 AUC -- even with the same\n",
    "parameters set.  This is true at least when using liblinear and using the\n",
    "default `max_iter` -- in fact, do not even input the default into the model; just leave\n",
    "the parameter out.  I found when explicitly using `max_iter=100`, the model would\n",
    "run at 0.83 AUC, but when leaving it out, it would run at either 0.83 AUC or\n",
    "0.999 AUC....  Very weird....\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Standardizer: (z - avg_trn) / std_trn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9313421256788208\n",
      "Val Accuracy: 0.8399280575539568\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9313421256788208\n",
      "Val Bal Accuracy: 0.7537673297166968\n",
      "-------------------\n",
      "Trn AUROC: 0.9313421256788208\n",
      "Val AUROC: 0.7537673297166967\n",
      "-------------------\n",
      "Trn AUPRC: 0.8792633015006821\n",
      "Val AUPRC: 0.01661337596589395\n",
      "-------------------\n",
      "Trn Precision Score: 0.939631650750341\n",
      "Val Precision Score: 0.9925898765965768\n",
      "-------------------\n",
      "Trn Recall Score: 0.9313421256788208\n",
      "Val Recall Score: 0.8399280575539568\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1112  177]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[465  88]\n",
      " [  1   2]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=57\n",
    "scaler='s'  # standardizer or normalizer or None\n",
    "solver='saga'  # liblinear, newton-cg, lbfgs, sag, saga\n",
    "penalty='l1'\n",
    "l1_ratio = None\n",
    "tol=1e-4  # default: 1e-4\n",
    "max_iter=5000 # default: 100\n",
    "cat_thresh=3\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\"\"\"\"\"\"\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "\n",
    "# Categoricals & Non-Categoricals\n",
    "# -- NOTE: this \"10\" split works on this data set, but so does \"3\"; oddly\n",
    "#      certain numbers will crash the AUC on some solvers...\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < cat_thresh]\n",
    "nots = [col for col in x_trn.columns if len(x[col].unique()) >= cat_thresh]\n",
    "\n",
    "\n",
    "# Scale non-cats (nots)\n",
    "## NOTE (2020-Jan-27): this \"0-1\" scaler sometimes hurts \n",
    "##   the AUC, while the standardizer does not...(?)\n",
    "##### NOTE2SELF:  when using SAG or SAGA solvers, you have to scale.\n",
    "if scaler is not None:\n",
    "    if scaler[0].lower()=='s':\n",
    "        print('Using Standardizer: (z - avg_trn) / std_trn')\n",
    "        avg_trn = x_trn[col].mean()\n",
    "        std_trn = x_trn[col].std()\n",
    "        scale = lambda z: (z - avg_trn) / std_trn\n",
    "    elif scaler[0].lower()=='n':\n",
    "        print('Using Normalizer: (z - min_trn) / (max_trn - min_trn)')\n",
    "        min_trn = x_trn[col].min()\n",
    "        max_trn = x_trn[col].max()\n",
    "        scale = lambda z: (z - min_trn) / (max_trn - min_trn)\n",
    "    for col in nots:\n",
    "        x_trn[col] = x_trn[col].map(scale)\n",
    "        x_val[col] = x_val[col].map(scale)\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "#  -- though it's easier to employ MIM before data split, if you want\n",
    "#     missing data recoded as 0 and missing cols only to have {0,1}, then\n",
    "#     it's safer to do it after rescaling\n",
    "'''\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x_trn[col+'_missing'] = [1  if item==-999999 else 0 for item in x_trn[col]]\n",
    "    x_trn[col] = x_trn[col].replace(-999999, 0) \n",
    "    x_val[col+'_missing'] = [1  if item==-999999 else 0 for item in x_val[col]]\n",
    "    x_val[col] = x_val[col].replace(-999999, 0)\n",
    "'''\n",
    "\n",
    "# SMOTE the training data\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "# - NOTE: liblinear solver does not support penalty='none'; instead\n",
    "#   try 'newton-cg', 'lbfgs', 'sag', and/or 'saga' \n",
    "model = LogisticRegression(\n",
    "    penalty=penalty, \n",
    "    solver=solver,\n",
    "    tol = tol,\n",
    "    max_iter = max_iter,\n",
    "    l1_ratio = l1_ratio,\n",
    ")\n",
    "#model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thought I was going crazy... So I went back to the simpler version of\n",
    "the most performant model -- the one without need to specify many parameters.\n",
    "\n",
    "At first, it seemed like this one was incredibly stable @ 0.999 AUC.  But then...every\n",
    "once in a while...it would hit 0.83 AUC.  I initially thought this had to do with\n",
    "me trying to add in a parameter, or remove one... But then the performance flip wouldn't\n",
    "happen every time...  \n",
    "\n",
    "So I just kept it simple and as-is -- then ran, ran, ran.  Yes, in truth, the AUC flips\n",
    "between 0.999 and 0.83...\n",
    "\n",
    "But, why?\n",
    "\n",
    "Well, there is a random state we didn't specify: the one in the model build.\n",
    "```\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "```\n",
    "\n",
    "So, simlarly to the random forest, there is a sensitivity to the opportunistic data split.\n",
    "\n",
    "One way we can control for this is by specifying the seed...\n",
    "\n",
    "But we can also keep looking at variable importances between 0.83 AUC and 0.99.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classication Metrics\n",
      "Trn Accuracy: 0.9992242048099301\n",
      "Val Accuracy: 0.9964028776978417\n",
      "-------------------\n",
      "Trn Bal Accuracy: 0.9992242048099302\n",
      "Val Bal Accuracy: 0.9981916817359855\n",
      "-------------------\n",
      "Trn AUROC: 0.9992242048099302\n",
      "Val AUROC: 0.9981916817359856\n",
      "-------------------\n",
      "Trn AUPRC: 0.9984508133230054\n",
      "Val AUPRC: 0.6\n",
      "-------------------\n",
      "Trn Precision Score: 0.9992254066615027\n",
      "Val Precision Score: 0.997841726618705\n",
      "-------------------\n",
      "Trn Recall Score: 0.9992242048099301\n",
      "Val Recall Score: 0.9964028776978417\n",
      "-------------------\n",
      "Trn Confusion:\n",
      " [[1287    2]\n",
      " [   0 1289]]\n",
      "Val Confusion:\n",
      " [[551   2]\n",
      " [  0   3]]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=57\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "\n",
    "# Fit Model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "# Model Metrics\n",
    "print('Classication Metrics')\n",
    "print('Trn Accuracy:',accuracy_score(y_trn, yp_trn))\n",
    "print('Val Accuracy:',accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Bal Accuracy:',balanced_accuracy_score(y_trn, yp_trn))\n",
    "print('Val Bal Accuracy:',balanced_accuracy_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUROC:',roc_auc_score(y_trn, yp_trn))\n",
    "print('Val AUROC:',roc_auc_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn AUPRC:',average_precision_score(y_trn, yp_trn))\n",
    "print('Val AUPRC:',average_precision_score(y_val, yp_val))\n",
    "print('-------------------')\n",
    "print('Trn Precision Score:',precision_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Precision Score:',precision_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Recall Score:',recall_score(y_trn, yp_trn, average='weighted'), )\n",
    "print('Val Recall Score:',recall_score(y_val, yp_val, average='weighted'))\n",
    "print('-------------------')\n",
    "print('Trn Confusion:\\n',confusion_matrix(y_trn, yp_trn), )\n",
    "print('Val Confusion:\\n',confusion_matrix(y_val, yp_val))\n",
    "print('-------------------')\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=57\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "while True:\n",
    "    # Fit Model\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(x_trn, y_trn)\n",
    "\n",
    "    # Make Predictions\n",
    "    yp_trn = model.predict(x_trn)\n",
    "    yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "    if roc_auc_score(y_val, yp_val) < 0.99:\n",
    "        feature_importance = abs(model.coef_[0])\n",
    "        feature_importance = feature_importance / feature_importance.max()\n",
    "        rank = np.flip(np.argsort(feature_importance))\n",
    "        imps = pd.DataFrame({\n",
    "            'ftr': x.columns[rank], \n",
    "            'imp': feature_importance[rank], \n",
    "            'sign': np.sign(model.coef_[0][rank])\n",
    "        })\n",
    "        imps83 = imps[0:30].reset_index().add_suffix('83')\n",
    "        break\n",
    "\n",
    "while True:\n",
    "    # Fit Model\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(x_trn, y_trn)\n",
    "\n",
    "    # Make Predictions\n",
    "    yp_trn = model.predict(x_trn)\n",
    "    yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "    if roc_auc_score(y_val, yp_val) >= 0.99:\n",
    "        feature_importance = abs(model.coef_[0])\n",
    "        feature_importance = feature_importance / feature_importance.max()\n",
    "        rank = np.flip(np.argsort(feature_importance))\n",
    "        imps = pd.DataFrame({\n",
    "            'ftr': x.columns[rank], \n",
    "            'imp': feature_importance[rank], \n",
    "            'sign': np.sign(model.coef_[0][rank])\n",
    "        })\n",
    "        imps99 = imps[0:30].reset_index().add_suffix('99')\n",
    "        break\n",
    "\n",
    "# Gini Importances\n",
    "#imp = zip(x.columns, rf.feature_importances_)\n",
    "#pd.DataFrame(imp, columns=['x_col','gini_importance']).\\\n",
    "#    sort_values(by='gini_importance',ascending=False).\\\n",
    "#    set_index(pd.Index(range(1,len(x.columns)+1)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "impdf = imps83.merge(imps99, left_on='ftr83', right_on='ftr99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do all var signs agree?  (Yes.)\n",
    "impdf.apply(lambda x: x.sign83 == x.sign99, axis=1).\\\n",
    "    to_frame('x').\\\n",
    "    query('x == False').\\\n",
    "    __len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all signs agree, we can technically drop the sign cols.\n",
    "impdf.drop(['sign83','sign99'], axis=1, inplace=True)\n",
    "\n",
    "# Also, since we merged on ftr names, we can drop one of the ftr cols\n",
    "impdf.drop('ftr99', axis=1, inplace=True)\n",
    "\n",
    "# Re-arrange cols for convenience\n",
    "impdf = impdf[['ftr83','index83','index99','imp83','imp99']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ftr83</th>\n",
       "      <th>index83</th>\n",
       "      <th>index99</th>\n",
       "      <th>imp83</th>\n",
       "      <th>imp99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880951</td>\n",
       "      <td>0.867162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_alc_missing</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.590449</td>\n",
       "      <td>0.488076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.513705</td>\n",
       "      <td>0.446070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.468823</td>\n",
       "      <td>0.339978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alcsmk_bt2_missing</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.329940</td>\n",
       "      <td>0.164006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adscmat_empl_comb4_missing</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318522</td>\n",
       "      <td>0.249989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alcsmk_t1_alc_missing</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.214925</td>\n",
       "      <td>0.268367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alcsmk_bt1_missing</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.206013</td>\n",
       "      <td>0.381125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.178417</td>\n",
       "      <td>0.297203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adscmat_parity</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.053313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adsmk_avgnumcighome_missing</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.154202</td>\n",
       "      <td>0.155460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adfetalgrowth_deviationindex_missing</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.121194</td>\n",
       "      <td>0.059479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.099219</td>\n",
       "      <td>0.084721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>0.092739</td>\n",
       "      <td>0.137262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>alcsmk_t2_alc</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.072492</td>\n",
       "      <td>0.086119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adedin_cycleid_all-10-15-20_db</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>0.046656</td>\n",
       "      <td>0.082349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alcsmk_t1_avecigs.wk</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.033093</td>\n",
       "      <td>0.025009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>alcsmk_t1_alc</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.022349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adelig_mat_age</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0.022057</td>\n",
       "      <td>0.033702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ftr83  index83  index99     imp83     imp99\n",
       "0           alcsmk_t2_avecigs.wk_missing        0        0  1.000000  1.000000\n",
       "1           alcsmk_t1_avecigs.wk_missing        1        1  0.880951  0.867162\n",
       "2                  alcsmk_t2_alc_missing        2        2  0.590449  0.488076\n",
       "3                  adscmat_educ_combohs4        3        3  0.513705  0.446070\n",
       "4           adscmat_grossincome7_missing        4        5  0.468823  0.339978\n",
       "5                     alcsmk_bt2_missing        5        9  0.329940  0.164006\n",
       "6             adscmat_empl_comb4_missing        6        8  0.318522  0.249989\n",
       "7                  alcsmk_t1_alc_missing        7        7  0.214925  0.268367\n",
       "8                     alcsmk_bt1_missing        8        4  0.206013  0.381125\n",
       "9                           adsc_gender2        9        6  0.178417  0.297203\n",
       "10                        adscmat_parity       10       16  0.165390  0.053313\n",
       "11           adsmk_avgnumcighome_missing       11       10  0.154202  0.155460\n",
       "12  adfetalgrowth_deviationindex_missing       12       15  0.121194  0.059479\n",
       "13                          admh_anemia3       13       13  0.099219  0.084721\n",
       "14                  adscmat_toiletwater2       14       11  0.092739  0.137262\n",
       "15                         alcsmk_t2_alc       15       12  0.072492  0.086119\n",
       "16        adedin_cycleid_all-10-15-20_db       16       14  0.046656  0.082349\n",
       "17                  alcsmk_t1_avecigs.wk       18       18  0.033093  0.025009\n",
       "18                         alcsmk_t1_alc       19       19  0.024607  0.022349\n",
       "19                        adelig_mat_age       20       17  0.022057  0.033702"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_diff</th>\n",
       "      <th>imp_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0.175112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.165934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.128845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.118786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.112077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.102373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.068533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0.067635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>0.061715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>0.053443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_diff  imp_diff\n",
       "0           8  0.175112\n",
       "1           5  0.165934\n",
       "2           4  0.128845\n",
       "3           9  0.118786\n",
       "4          10  0.112077\n",
       "5           2  0.102373\n",
       "6           6  0.068533\n",
       "7           3  0.067635\n",
       "8          12  0.061715\n",
       "9           7  0.053443"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_diff = impdf.apply(lambda x: x.imp83 - x.imp99, axis=1).\\\n",
    "    map(lambda x: abs(x)).\\\n",
    "    to_frame('imp').\\\n",
    "    sort_values(by='imp', ascending=False).\\\n",
    "    reset_index().\\\n",
    "    add_suffix('_diff')\n",
    "imp_diff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ftr83</th>\n",
       "      <th>index83</th>\n",
       "      <th>index99</th>\n",
       "      <th>imp83</th>\n",
       "      <th>imp99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alcsmk_bt1_missing</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.206013</td>\n",
       "      <td>0.381125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alcsmk_bt2_missing</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.329940</td>\n",
       "      <td>0.164006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.468823</td>\n",
       "      <td>0.339978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.178417</td>\n",
       "      <td>0.297203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adscmat_parity</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.053313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_alc_missing</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.590449</td>\n",
       "      <td>0.488076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adscmat_empl_comb4_missing</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.318522</td>\n",
       "      <td>0.249989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.513705</td>\n",
       "      <td>0.446070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adfetalgrowth_deviationindex_missing</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.121194</td>\n",
       "      <td>0.059479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alcsmk_t1_alc_missing</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.214925</td>\n",
       "      <td>0.268367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ftr83  index83  index99     imp83     imp99\n",
       "8                     alcsmk_bt1_missing        8        4  0.206013  0.381125\n",
       "5                     alcsmk_bt2_missing        5        9  0.329940  0.164006\n",
       "4           adscmat_grossincome7_missing        4        5  0.468823  0.339978\n",
       "9                           adsc_gender2        9        6  0.178417  0.297203\n",
       "10                        adscmat_parity       10       16  0.165390  0.053313\n",
       "2                  alcsmk_t2_alc_missing        2        2  0.590449  0.488076\n",
       "6             adscmat_empl_comb4_missing        6        8  0.318522  0.249989\n",
       "3                  adscmat_educ_combohs4        3        3  0.513705  0.446070\n",
       "12  adfetalgrowth_deviationindex_missing       12       15  0.121194  0.059479\n",
       "7                  alcsmk_t1_alc_missing        7        7  0.214925  0.268367"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impdf.iloc[imp_diff.index_diff].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute coeff strengths instead of relative....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=57\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "while True:\n",
    "    # Fit Model\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(x_trn, y_trn)\n",
    "\n",
    "    # Make Predictions\n",
    "    yp_trn = model.predict(x_trn)\n",
    "    yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "    if roc_auc_score(y_val, yp_val) < 0.99:\n",
    "        feature_importance = abs(model.coef_[0])\n",
    "        rank = np.flip(np.argsort(feature_importance))\n",
    "        imps = pd.DataFrame({\n",
    "            'ftr': x.columns[rank], \n",
    "            'imp': feature_importance[rank], \n",
    "            'sign': np.sign(model.coef_[0][rank])\n",
    "        })\n",
    "        imps83 = imps[0:30].reset_index().add_suffix('83')\n",
    "        break\n",
    "\n",
    "while True:\n",
    "    # Fit Model\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "    model.fit(x_trn, y_trn)\n",
    "\n",
    "    # Make Predictions\n",
    "    yp_trn = model.predict(x_trn)\n",
    "    yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "    if roc_auc_score(y_val, yp_val) >= 0.99:\n",
    "        feature_importance = abs(model.coef_[0])\n",
    "        rank = np.flip(np.argsort(feature_importance))\n",
    "        imps = pd.DataFrame({\n",
    "            'ftr': x.columns[rank], \n",
    "            'imp': feature_importance[rank], \n",
    "            'sign': np.sign(model.coef_[0][rank])\n",
    "        })\n",
    "        imps99 = imps[0:30].reset_index().add_suffix('99')\n",
    "        break\n",
    "\n",
    "        \n",
    "impdf = imps83.merge(imps99, left_on='ftr83', right_on='ftr99')\n",
    "\n",
    "# Since all signs agree, we can technically drop the sign cols.\n",
    "impdf.drop(['sign83','sign99'], axis=1, inplace=True)\n",
    "\n",
    "# Also, since we merged on ftr names, we can drop one of the ftr cols\n",
    "impdf.drop('ftr99', axis=1, inplace=True)\n",
    "\n",
    "# Re-arrange cols for convenience\n",
    "impdf = impdf[['ftr83','index83','index99','imp83','imp99']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "impdf['imp_diff']    = impdf.imp99 - impdf.imp83\n",
    "impdf['imp_diff_pc'] = np.round(100 * (impdf.imp99 - impdf.imp83) / impdf.imp83, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ftr83</th>\n",
       "      <th>index83</th>\n",
       "      <th>index99</th>\n",
       "      <th>imp83</th>\n",
       "      <th>imp99</th>\n",
       "      <th>imp_diff</th>\n",
       "      <th>imp_diff_pc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.642583</td>\n",
       "      <td>4.266689</td>\n",
       "      <td>-0.375894</td>\n",
       "      <td>-8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.126004</td>\n",
       "      <td>4.659868</td>\n",
       "      <td>0.533864</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_alc_missing</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.733655</td>\n",
       "      <td>1.465266</td>\n",
       "      <td>-1.268390</td>\n",
       "      <td>-46.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.245945</td>\n",
       "      <td>2.169745</td>\n",
       "      <td>-0.076200</td>\n",
       "      <td>-3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.948538</td>\n",
       "      <td>1.763943</td>\n",
       "      <td>-0.184595</td>\n",
       "      <td>-9.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alcsmk_bt1_missing</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.590121</td>\n",
       "      <td>1.341765</td>\n",
       "      <td>-0.248356</td>\n",
       "      <td>-15.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adscmat_empl_comb4_missing</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1.350185</td>\n",
       "      <td>1.272548</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>-5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.083397</td>\n",
       "      <td>1.298255</td>\n",
       "      <td>0.214859</td>\n",
       "      <td>19.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alcsmk_t1_alc_missing</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.941104</td>\n",
       "      <td>2.429169</td>\n",
       "      <td>1.488065</td>\n",
       "      <td>158.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alcsmk_bt2_missing</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.910539</td>\n",
       "      <td>1.035200</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>13.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adsmk_avgnumcighome_missing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.694368</td>\n",
       "      <td>0.764862</td>\n",
       "      <td>0.070494</td>\n",
       "      <td>10.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adscmat_parity</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0.544355</td>\n",
       "      <td>0.331072</td>\n",
       "      <td>-0.213283</td>\n",
       "      <td>-39.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.525774</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.086423</td>\n",
       "      <td>16.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adfetalgrowth_deviationindex_missing</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0.438284</td>\n",
       "      <td>0.342060</td>\n",
       "      <td>-0.096224</td>\n",
       "      <td>-21.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.428322</td>\n",
       "      <td>0.419982</td>\n",
       "      <td>-0.008340</td>\n",
       "      <td>-1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>alcsmk_t2_alc</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.367644</td>\n",
       "      <td>0.406492</td>\n",
       "      <td>0.038848</td>\n",
       "      <td>10.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adedin_cycleid_all-10-15-20_db</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.234628</td>\n",
       "      <td>0.318355</td>\n",
       "      <td>0.083727</td>\n",
       "      <td>35.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alcsmk_t1_avecigs.wk</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.126485</td>\n",
       "      <td>-0.024983</td>\n",
       "      <td>-16.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adelig_mat_age</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.158688</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>22.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>alcsmk_t1_alc</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>0.108974</td>\n",
       "      <td>0.112195</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>2.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ftr83  index83  index99     imp83  \\\n",
       "0           alcsmk_t2_avecigs.wk_missing        0        1  4.642583   \n",
       "1           alcsmk_t1_avecigs.wk_missing        1        0  4.126004   \n",
       "2                  alcsmk_t2_alc_missing        2        5  2.733655   \n",
       "3                  adscmat_educ_combohs4        3        3  2.245945   \n",
       "4           adscmat_grossincome7_missing        4        4  1.948538   \n",
       "5                     alcsmk_bt1_missing        5        6  1.590121   \n",
       "6             adscmat_empl_comb4_missing        6        8  1.350185   \n",
       "7                           adsc_gender2        7        7  1.083397   \n",
       "8                  alcsmk_t1_alc_missing        8        2  0.941104   \n",
       "9                     alcsmk_bt2_missing        9        9  0.910539   \n",
       "10           adsmk_avgnumcighome_missing       10       10  0.694368   \n",
       "11                        adscmat_parity       11       15  0.544355   \n",
       "12                  adscmat_toiletwater2       12       11  0.525774   \n",
       "13  adfetalgrowth_deviationindex_missing       13       14  0.438284   \n",
       "14                          admh_anemia3       14       12  0.428322   \n",
       "15                         alcsmk_t2_alc       15       13  0.367644   \n",
       "16        adedin_cycleid_all-10-15-20_db       16       16  0.234628   \n",
       "17                  alcsmk_t1_avecigs.wk       18       18  0.151468   \n",
       "18                        adelig_mat_age       19       17  0.129850   \n",
       "19                         alcsmk_t1_alc       20       19  0.108974   \n",
       "\n",
       "       imp99  imp_diff  imp_diff_pc  \n",
       "0   4.266689 -0.375894        -8.10  \n",
       "1   4.659868  0.533864        12.94  \n",
       "2   1.465266 -1.268390       -46.40  \n",
       "3   2.169745 -0.076200        -3.39  \n",
       "4   1.763943 -0.184595        -9.47  \n",
       "5   1.341765 -0.248356       -15.62  \n",
       "6   1.272548 -0.077637        -5.75  \n",
       "7   1.298255  0.214859        19.83  \n",
       "8   2.429169  1.488065       158.12  \n",
       "9   1.035200  0.124661        13.69  \n",
       "10  0.764862  0.070494        10.15  \n",
       "11  0.331072 -0.213283       -39.18  \n",
       "12  0.612197  0.086423        16.44  \n",
       "13  0.342060 -0.096224       -21.95  \n",
       "14  0.419982 -0.008340        -1.95  \n",
       "15  0.406492  0.038848        10.57  \n",
       "16  0.318355  0.083727        35.69  \n",
       "17  0.126485 -0.024983       -16.49  \n",
       "18  0.158688  0.028838        22.21  \n",
       "19  0.112195  0.003221         2.96  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ftr83</th>\n",
       "      <th>index83</th>\n",
       "      <th>index99</th>\n",
       "      <th>imp83</th>\n",
       "      <th>imp99</th>\n",
       "      <th>imp_diff</th>\n",
       "      <th>imp_diff_pc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcsmk_t1_avecigs.wk_missing</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.126004</td>\n",
       "      <td>4.659868</td>\n",
       "      <td>0.533864</td>\n",
       "      <td>12.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alcsmk_t2_avecigs.wk_missing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.642583</td>\n",
       "      <td>4.266689</td>\n",
       "      <td>-0.375894</td>\n",
       "      <td>-8.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alcsmk_t1_alc_missing</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.941104</td>\n",
       "      <td>2.429169</td>\n",
       "      <td>1.488065</td>\n",
       "      <td>158.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adscmat_educ_combohs4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.245945</td>\n",
       "      <td>2.169745</td>\n",
       "      <td>-0.076200</td>\n",
       "      <td>-3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adscmat_grossincome7_missing</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.948538</td>\n",
       "      <td>1.763943</td>\n",
       "      <td>-0.184595</td>\n",
       "      <td>-9.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcsmk_t2_alc_missing</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2.733655</td>\n",
       "      <td>1.465266</td>\n",
       "      <td>-1.268390</td>\n",
       "      <td>-46.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alcsmk_bt1_missing</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.590121</td>\n",
       "      <td>1.341765</td>\n",
       "      <td>-0.248356</td>\n",
       "      <td>-15.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adsc_gender2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.083397</td>\n",
       "      <td>1.298255</td>\n",
       "      <td>0.214859</td>\n",
       "      <td>19.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adscmat_empl_comb4_missing</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1.350185</td>\n",
       "      <td>1.272548</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>-5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alcsmk_bt2_missing</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.910539</td>\n",
       "      <td>1.035200</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>13.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adsmk_avgnumcighome_missing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.694368</td>\n",
       "      <td>0.764862</td>\n",
       "      <td>0.070494</td>\n",
       "      <td>10.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adscmat_toiletwater2</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>0.525774</td>\n",
       "      <td>0.612197</td>\n",
       "      <td>0.086423</td>\n",
       "      <td>16.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>admh_anemia3</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.428322</td>\n",
       "      <td>0.419982</td>\n",
       "      <td>-0.008340</td>\n",
       "      <td>-1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>alcsmk_t2_alc</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.367644</td>\n",
       "      <td>0.406492</td>\n",
       "      <td>0.038848</td>\n",
       "      <td>10.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adfetalgrowth_deviationindex_missing</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0.438284</td>\n",
       "      <td>0.342060</td>\n",
       "      <td>-0.096224</td>\n",
       "      <td>-21.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adscmat_parity</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0.544355</td>\n",
       "      <td>0.331072</td>\n",
       "      <td>-0.213283</td>\n",
       "      <td>-39.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adedin_cycleid_all-10-15-20_db</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.234628</td>\n",
       "      <td>0.318355</td>\n",
       "      <td>0.083727</td>\n",
       "      <td>35.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adelig_mat_age</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.158688</td>\n",
       "      <td>0.028838</td>\n",
       "      <td>22.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>alcsmk_t1_avecigs.wk</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.126485</td>\n",
       "      <td>-0.024983</td>\n",
       "      <td>-16.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>alcsmk_t1_alc</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>0.108974</td>\n",
       "      <td>0.112195</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>2.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ftr83  index83  index99     imp83  \\\n",
       "1           alcsmk_t1_avecigs.wk_missing        1        0  4.126004   \n",
       "0           alcsmk_t2_avecigs.wk_missing        0        1  4.642583   \n",
       "8                  alcsmk_t1_alc_missing        8        2  0.941104   \n",
       "3                  adscmat_educ_combohs4        3        3  2.245945   \n",
       "4           adscmat_grossincome7_missing        4        4  1.948538   \n",
       "2                  alcsmk_t2_alc_missing        2        5  2.733655   \n",
       "5                     alcsmk_bt1_missing        5        6  1.590121   \n",
       "7                           adsc_gender2        7        7  1.083397   \n",
       "6             adscmat_empl_comb4_missing        6        8  1.350185   \n",
       "9                     alcsmk_bt2_missing        9        9  0.910539   \n",
       "10           adsmk_avgnumcighome_missing       10       10  0.694368   \n",
       "12                  adscmat_toiletwater2       12       11  0.525774   \n",
       "14                          admh_anemia3       14       12  0.428322   \n",
       "15                         alcsmk_t2_alc       15       13  0.367644   \n",
       "13  adfetalgrowth_deviationindex_missing       13       14  0.438284   \n",
       "11                        adscmat_parity       11       15  0.544355   \n",
       "16        adedin_cycleid_all-10-15-20_db       16       16  0.234628   \n",
       "18                        adelig_mat_age       19       17  0.129850   \n",
       "17                  alcsmk_t1_avecigs.wk       18       18  0.151468   \n",
       "19                         alcsmk_t1_alc       20       19  0.108974   \n",
       "\n",
       "       imp99  imp_diff  imp_diff_pc  \n",
       "1   4.659868  0.533864        12.94  \n",
       "0   4.266689 -0.375894        -8.10  \n",
       "8   2.429169  1.488065       158.12  \n",
       "3   2.169745 -0.076200        -3.39  \n",
       "4   1.763943 -0.184595        -9.47  \n",
       "2   1.465266 -1.268390       -46.40  \n",
       "5   1.341765 -0.248356       -15.62  \n",
       "7   1.298255  0.214859        19.83  \n",
       "6   1.272548 -0.077637        -5.75  \n",
       "9   1.035200  0.124661        13.69  \n",
       "10  0.764862  0.070494        10.15  \n",
       "12  0.612197  0.086423        16.44  \n",
       "14  0.419982 -0.008340        -1.95  \n",
       "15  0.406492  0.038848        10.57  \n",
       "13  0.342060 -0.096224       -21.95  \n",
       "11  0.331072 -0.213283       -39.18  \n",
       "16  0.318355  0.083727        35.69  \n",
       "18  0.158688  0.028838        22.21  \n",
       "17  0.126485 -0.024983       -16.49  \n",
       "19  0.112195  0.003221         2.96  "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impdf.sort_values(by='imp99', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/10042388/program-that-convert-html-to-image\n",
    "    \n",
    "https://docs.python.org/2/library/subprocess.html\n",
    "\n",
    "I have been trying to convert a pandas table to an image automatically... which I do below, but\n",
    "it doesn't really look good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "impdf.sort_values(by='imp99', ascending=False).head(20).to_html('../reports/figures/test.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.call('wkhtmltoimage -f png --width 0 ../reports/figures/test.html ../reports/figures/test.png', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, make this all more deterministic...\n",
    "\n",
    "* Add `random_state` to logistic regression and pass it the seed that everything\n",
    "else gets passed\n",
    "* Find a seed that produces AUC 0.99\n",
    "* Find another seed that produces AUC 0.83\n",
    "\n",
    "I'll do this in the next JNB:  `07_KU_Random-Seed-Dependence-Demystified`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:512: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:514: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n",
      "/Users/kevinurban/miniconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:518: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not x.flags.writeable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.4972875226039783\n"
     ]
    }
   ],
   "source": [
    "# INPUTS\n",
    "target = 'adsc_dlvrybefore28wks'\n",
    "data = toif1pwr_demotoif1pwr_df\n",
    "seed=83\n",
    "# AUC 50: 83\n",
    "# AUC 66: 5, 17, 53, 61, 67, 73, 79, 89, 127\n",
    "# AUC 83: 2, 11, 19, 31, 37, 41, 43, 71, 97, 101, 103, 107, 109\n",
    "# AUC 99: 3, 7, 13, 23, 29, 47, 59, 113, 131\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Define Scenario Data (only Demo/Clinical)\n",
    "x = data.drop(target_cols+['sensor_toi_f1_mvt_pwr_mvt41'], axis=1).copy()\n",
    "y = data[[target]].copy()\n",
    "\n",
    "# Only keep records with non-missing target value\n",
    "valid_target_index = y[target].replace(-999999, np.nan).dropna().index\n",
    "x = x.loc[valid_target_index,:]\n",
    "y = y.loc[valid_target_index,:]\n",
    "\n",
    "\n",
    "# Employ Missing Indicator Method\n",
    "missing_cols = [\n",
    "    item for item in \n",
    "    x.replace(-999999,np.nan).isna().sum().to_frame('missing').query('missing > 0').index.tolist()\n",
    "]\n",
    "for col in missing_cols:\n",
    "    x[col+'_missing'] = [1  if item==-999999 else 0 for item in x[col]]\n",
    "    x[col] = x[col].replace(-999999, 0)\n",
    "\n",
    "    \n",
    "# Split Data\n",
    "x_trn, x_val, y_trn, y_val = train_test_split(x, y, train_size=0.7, stratify=y, random_state=seed)\n",
    "\n",
    "# SMOTE the training data\n",
    "cats = [idx for idx,col in enumerate(x_trn.columns) if len(x[col].unique()) < 10]\n",
    "sm = SMOTENC(cats, random_state=seed)\n",
    "x_trn, y_trn = sm.fit_resample(x_trn.values, y_trn.values.ravel())\n",
    "\n",
    "# Fit Model\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', \n",
    "                          random_state=seed)\n",
    "model.fit(x_trn, y_trn)\n",
    "\n",
    "# Make Predictions\n",
    "yp_trn = model.predict(x_trn)\n",
    "yp_val = model.predict(x_val)\n",
    "\n",
    "\n",
    "print('AUC:', roc_auc_score(y_val, yp_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
